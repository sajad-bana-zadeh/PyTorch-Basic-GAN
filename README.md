# PyTorch Basic GAN

This project is a basic implementation of a Generative Adversarial Network (GAN) using PyTorch. It trains a Generator and a Discriminator on the MNIST dataset to generate realistic handwritten digit images. This implementation is beginner-friendly and focuses on the core concepts of GANs using fully connected (dense) networks.

## Overview

A GAN consists of two neural networks competing against each other:

- **Generator (G):** Takes random noise as input and generates fake images resembling real data.
- **Discriminator (D):** Attempts to distinguish between real images (from the MNIST dataset) and fake images (from the generator).

During training:

- The discriminator learns to better classify real and fake images.
- The generator learns to create images that can fool the discriminator.
  
The training continues until the generator produces realistic images indistinguishable from the real dataset.

## Project Structure

- `basic_gan.ipynb` — Jupyter notebook containing:
  - Data preparation using MNIST normalized to $$[-1, 1]$$
  - Definition of Generator and Discriminator as fully connected networks
  - Training loop for 100 epochs with Adam optimizers and BCE loss
  - Image generation and visualization
  
*(Note: The code is currently self-contained within the notebook.)*

## Installation

To run this notebook, you need Python installed along with PyTorch, torchvision, and matplotlib.

You can install the required packages via pip:

```bash
pip install torch torchvision matplotlib
```

## Usage

Run the notebook `basic_gan.ipynb`. It will:

- Download and preprocess the MNIST dataset
- Initialize and train Generator and Discriminator for 100 epochs with a batch size of 64
- Use Binary Cross Entropy loss and Adam optimizers (lr=0.0002, betas=(0.5, 0.999))
- Print discriminator and generator losses every 100 steps during training
- Generate and display a grid of 16 images produced by the trained generator
  
---

## Key Implementation Details

### 1. Dataset Preparation

- MNIST handwritten digit dataset is normalized to $$[-1, 1]$$ to stabilize GAN training
- DataLoader with batch size 64 shuffles training data for each epoch

### 2. Generator Network

- Input: noise vector of dimension 100
- Structure: two fully connected layers
  - Linear(100 → 128) + ReLU
  - Linear(128 → 784) + Tanh
- Output: 28 × 28 grayscale image with pixel values between -1 and 1

### 3. Discriminator Network

- Input: 28 × 28 image, flattened to 784 features
- Structure: two fully connected layers
  - Linear(784 → 128) + LeakyReLU(0.2)
  - Linear(128 → 1) + Sigmoid
- Output: probability (0 to 1) of image being real

### 4. Loss and Optimization

- Loss: Binary Cross Entropy (BCELoss)
- Optimizer: Adam with learning rate 0.0002 and betas (0.5, 0.999)

### 5. Training Loop

- For each batch:
  - Train discriminator on real images (label = 1) and fake images generated by the generator (label = 0)
  - Train generator to produce images that fool the discriminator (label = 1 for fake images)
- Losses are logged every 100 batches

## Results

Generated images improve over epochs, producing digits increasingly similar to real handwritten digits from MNIST. Visualization of generated samples is shown after training.

## License

This project is licensed under the GNU GENERAL PUBLIC LICENSE Version 3 (GPL-3.0). See the LICENSE file for details.

## Acknowledgements

- Inspired by PyTorch tutorials and foundational GAN research  
- Thanks to the PyTorch community for invaluable resources

Feel free to use, modify, and extend this basic GAN implementation for your own projects or learning!
