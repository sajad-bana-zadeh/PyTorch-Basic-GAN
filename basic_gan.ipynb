{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72241b46-34c8-4d2d-a682-6f0b0372a7da",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c65043f-82f2-4115-abec-6d4bf48e6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5370775-9674-493c-9369-85a2abd835e4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7b516-dab4-4d00-90a5-9df7d9dcfd01",
   "metadata": {},
   "source": [
    "## 2. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ee324-45d5-466e-a8b7-7bdb0e4f0098",
   "metadata": {},
   "source": [
    "- Use MNIST as a typical example, normalize it to [-1, 1] for better GAN performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36a4d3-08a3-448f-b4bd-926a557b4640",
   "metadata": {},
   "source": [
    "This command creates a **data transformation pipeline** typically used in image preprocessing for deep learning tasks with libraries like PyTorch.\n",
    "\n",
    "Here’s what the command does, step-by-step:\n",
    "\n",
    "1. **`transforms.Compose([...])`**  \n",
    "   This wraps multiple transformations into one. It applies them sequentially to the input data.\n",
    "\n",
    "2. **`transforms.ToTensor()`**  \n",
    "   Converts a PIL image or NumPy ndarray into a PyTorch tensor. It also scales pixel values from the range  to [0.0, 1.0].\n",
    "\n",
    "3. **`transforms.Normalize(mean=(0.5,), std=(0.5,))`**  \n",
    "   Normalizes the tensor by subtracting the mean and dividing by the standard deviation for each channel. Here, the single value `(0.5,)` corresponds to one channel (like grayscale).\n",
    "\n",
    "   Since the pixel values are between 0 and 1 after `ToTensor()`, normalization with mean=0.5 and std=0.5 converts them roughly to the range [-1, 1] using this formula:\n",
    "\n",
    "   $$\n",
    "   \\text{normalized\\_pixel} = \\frac{\\text{pixel} - 0.5}{0.5}\n",
    "   $$\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- **Convert images to tensors** (making them compatible with PyTorch models).\n",
    "- **Normalize pixel intensity values** so the model trains more effectively (helps with convergence and stabilizes training by standardizing input).\n",
    "\n",
    "This transformation is common in training neural networks on datasets like MNIST or grayscale images since it processes the image into a normalized tensor input suitable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce9e330-4d34-4f5f-a08b-c772756154f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dec174-af00-49c9-a7e4-59bfd3b388c2",
   "metadata": {},
   "source": [
    "This command creates a **training dataset object** for the MNIST dataset using PyTorch’s `torchvision.datasets` module.\n",
    "\n",
    "Here’s a breakdown of the command:\n",
    "\n",
    "- **`datasets.MNIST(...)`**  \n",
    "  This loads the MNIST handwritten digit dataset, which contains grayscale images of digits (0–9).\n",
    "\n",
    "- **`root='./data'`**  \n",
    "  Specifies the directory where the MNIST data will be stored or loaded from. If the data is not already present, it will be downloaded to this folder.\n",
    "\n",
    "- **`train=True`**  \n",
    "  Indicates that this dataset object should load the **training split** of MNIST (60,000 images). If set to `False`, it would load the test split (10,000 images).\n",
    "\n",
    "- **`download=True`**  \n",
    "  If the dataset isn’t already present in the specified `root` folder, PyTorch will automatically download it from the internet.\n",
    "\n",
    "- **`transform=transform`**  \n",
    "  Applies the previously defined transformations (such as conversion to tensor and normalization) to every image when it is loaded. This preprocesses the images so they're ready for training.\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- It **loads** the MNIST training dataset, downloading it if necessary.\n",
    "- It **applies the defined transformations** (like converting images to tensors and normalizing) on the data.\n",
    "- It prepares the dataset in a format suitable for training a PyTorch model.\n",
    "\n",
    "In short, this command **sets up your training data pipeline** for feeding images into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c37f00c-b8eb-4dd3-89c4-f3e0549ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861b99f-1c6b-48e2-9614-d13229ab29b0",
   "metadata": {},
   "source": [
    "This command creates a **DataLoader** object from the previously defined `train_dataset` (MNIST training dataset in this case).\n",
    "\n",
    "Here’s what it does:\n",
    "\n",
    "- **`DataLoader(train_dataset, batch_size=64, shuffle=True)`**  \n",
    "  This wraps the dataset in a convenient iterator for training machine learning models.\n",
    "\n",
    "- **`batch_size=64`**  \n",
    "  Specifies that data will be loaded in batches of 64 samples. Instead of passing one image at a time, the model receives 64 images in each training step, which is efficient and stabilizes training.\n",
    "\n",
    "- **`shuffle=True`**  \n",
    "  Means the dataset will be shuffled at the start of each epoch. Shuffling helps the model see the data in a different order each time, which improves training by reducing overfitting and increasing generalization.\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- To **batch the dataset** enabling efficient processing and training.\n",
    "- To **shuffle the training data** so the model does not learn the order of data, which promotes better generalization.\n",
    "- To provide an **easily iterable object** that returns batches of transformed images and their labels, ready for training.\n",
    "\n",
    "In summary, this command sets up the training pipeline to feed the model mini-batches of shuffled data for training. It abstracts away loading and batching complexities so the training loop can run smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4633ff2d-baaa-423f-ac15-203133daf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e6fc8-79e5-49a7-819d-dfea2305add4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90aef-9dcf-4771-a746-3cddb2aeef12",
   "metadata": {},
   "source": [
    "## 3. Define the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ccdc1-37d4-4995-a7df-8723362df7ff",
   "metadata": {},
   "source": [
    "- **Generator:** Turns noisy random input into a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e54d409-c5dc-4216-8b77-fbe446b48c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Tanh()  # Output between -1 and 1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830491e8-95b4-402a-9fec-3b650252390d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbc295-2c83-4520-b80b-cf12ddc17295",
   "metadata": {},
   "source": [
    "## 4. Define the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840e5da-95c4-49de-a52e-d4a11bed1407",
   "metadata": {},
   "source": [
    "- **Discriminator:** Predicts if input image is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1d4f7e-6d7a-46c7-a4de-e836a765a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4681a0-ee7b-4b86-9601-8adb38e2e663",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03909a-5eae-4a03-95d8-7da29473587e",
   "metadata": {},
   "source": [
    "## 5. Initialize networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe443f-84ce-433a-a5c4-2f6692ea3a0b",
   "metadata": {},
   "source": [
    "- Set your device (GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07d63f5-cd9c-470f-a0e0-c19b44660adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4736ed-9685-4538-a285-6a28827e9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "generator = Generator(noise_dim).to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cbff4-ab6f-477a-a416-e0cc17442e4d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fbf88-1b5e-4c9a-91e4-51d6b0c399af",
   "metadata": {},
   "source": [
    "## 6. Set Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8caf94d6-a5f4-4572-bcd8-9dddb9abc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9138f80c-c0ab-4cb8-b911-07c0d8a251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cea54115-7ea5-41b1-bcbc-79ee4e68a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b57d33-7464-493d-8f66-63870626967c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5bfca-e65f-406d-81b9-67e6e958270c",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3e11b-b001-45ce-b1f0-b3066be0f6ee",
   "metadata": {},
   "source": [
    "For each batch:\n",
    "- Train D: on real images (label=1) and fake images (label=0).\n",
    "- Train G: try to make D classify generated images as real (label=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967f2a14-7ad7-49a1-bb21-ba8c77103d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee96778e-d408-4b2c-a166-edf6ed4b4347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [0/938], D Loss: 1.3005, G Loss: 0.6638\n",
      "Epoch [1/25], Step [100/938], D Loss: 1.0342, G Loss: 0.5612\n",
      "Epoch [1/25], Step [200/938], D Loss: 1.1112, G Loss: 0.6028\n",
      "Epoch [1/25], Step [300/938], D Loss: 1.1625, G Loss: 0.6145\n",
      "Epoch [1/25], Step [400/938], D Loss: 1.1095, G Loss: 0.7175\n",
      "Epoch [1/25], Step [500/938], D Loss: 0.8854, G Loss: 1.0585\n",
      "Epoch [1/25], Step [600/938], D Loss: 0.9202, G Loss: 1.0275\n",
      "Epoch [1/25], Step [700/938], D Loss: 0.8455, G Loss: 1.0493\n",
      "Epoch [1/25], Step [800/938], D Loss: 0.7722, G Loss: 1.1716\n",
      "Epoch [1/25], Step [900/938], D Loss: 0.7347, G Loss: 1.1164\n",
      "Epoch [2/25], Step [0/938], D Loss: 0.7377, G Loss: 1.1995\n",
      "Epoch [2/25], Step [100/938], D Loss: 0.8752, G Loss: 1.1499\n",
      "Epoch [2/25], Step [200/938], D Loss: 0.7690, G Loss: 1.1292\n",
      "Epoch [2/25], Step [300/938], D Loss: 0.8072, G Loss: 1.1733\n",
      "Epoch [2/25], Step [400/938], D Loss: 0.7594, G Loss: 1.3397\n",
      "Epoch [2/25], Step [500/938], D Loss: 0.7426, G Loss: 1.2103\n",
      "Epoch [2/25], Step [600/938], D Loss: 0.8812, G Loss: 1.1189\n",
      "Epoch [2/25], Step [700/938], D Loss: 0.8396, G Loss: 1.2335\n",
      "Epoch [2/25], Step [800/938], D Loss: 0.9586, G Loss: 0.9505\n",
      "Epoch [2/25], Step [900/938], D Loss: 0.9032, G Loss: 1.2153\n",
      "Epoch [3/25], Step [0/938], D Loss: 0.9369, G Loss: 0.9664\n",
      "Epoch [3/25], Step [100/938], D Loss: 0.7843, G Loss: 1.1055\n",
      "Epoch [3/25], Step [200/938], D Loss: 0.7303, G Loss: 1.3283\n",
      "Epoch [3/25], Step [300/938], D Loss: 0.7370, G Loss: 1.3043\n",
      "Epoch [3/25], Step [400/938], D Loss: 0.5769, G Loss: 1.4071\n",
      "Epoch [3/25], Step [500/938], D Loss: 0.5847, G Loss: 1.7586\n",
      "Epoch [3/25], Step [600/938], D Loss: 0.6631, G Loss: 2.1980\n",
      "Epoch [3/25], Step [700/938], D Loss: 0.6024, G Loss: 1.7314\n",
      "Epoch [3/25], Step [800/938], D Loss: 0.7193, G Loss: 1.8757\n",
      "Epoch [3/25], Step [900/938], D Loss: 0.8377, G Loss: 1.7050\n",
      "Epoch [4/25], Step [0/938], D Loss: 0.7921, G Loss: 1.0630\n",
      "Epoch [4/25], Step [100/938], D Loss: 0.7217, G Loss: 1.7653\n",
      "Epoch [4/25], Step [200/938], D Loss: 0.8409, G Loss: 0.9607\n",
      "Epoch [4/25], Step [300/938], D Loss: 0.7568, G Loss: 1.9521\n",
      "Epoch [4/25], Step [400/938], D Loss: 0.8191, G Loss: 1.5725\n",
      "Epoch [4/25], Step [500/938], D Loss: 0.7854, G Loss: 1.4722\n",
      "Epoch [4/25], Step [600/938], D Loss: 0.7500, G Loss: 1.2720\n",
      "Epoch [4/25], Step [700/938], D Loss: 0.9236, G Loss: 2.2401\n",
      "Epoch [4/25], Step [800/938], D Loss: 0.6354, G Loss: 1.4934\n",
      "Epoch [4/25], Step [900/938], D Loss: 0.7599, G Loss: 1.4728\n",
      "Epoch [5/25], Step [0/938], D Loss: 0.6990, G Loss: 1.3940\n",
      "Epoch [5/25], Step [100/938], D Loss: 0.6655, G Loss: 1.7302\n",
      "Epoch [5/25], Step [200/938], D Loss: 0.8173, G Loss: 1.3229\n",
      "Epoch [5/25], Step [300/938], D Loss: 0.6822, G Loss: 1.2387\n",
      "Epoch [5/25], Step [400/938], D Loss: 0.6711, G Loss: 2.1441\n",
      "Epoch [5/25], Step [500/938], D Loss: 0.7800, G Loss: 2.3304\n",
      "Epoch [5/25], Step [600/938], D Loss: 0.6672, G Loss: 1.5813\n",
      "Epoch [5/25], Step [700/938], D Loss: 0.6225, G Loss: 2.2089\n",
      "Epoch [5/25], Step [800/938], D Loss: 0.6297, G Loss: 1.9208\n",
      "Epoch [5/25], Step [900/938], D Loss: 0.5357, G Loss: 1.4785\n",
      "Epoch [6/25], Step [0/938], D Loss: 0.7477, G Loss: 1.6569\n",
      "Epoch [6/25], Step [100/938], D Loss: 0.5665, G Loss: 1.5315\n",
      "Epoch [6/25], Step [200/938], D Loss: 0.7563, G Loss: 1.6682\n",
      "Epoch [6/25], Step [300/938], D Loss: 0.6129, G Loss: 1.5926\n",
      "Epoch [6/25], Step [400/938], D Loss: 0.6625, G Loss: 1.6023\n",
      "Epoch [6/25], Step [500/938], D Loss: 0.8702, G Loss: 0.9405\n",
      "Epoch [6/25], Step [600/938], D Loss: 0.9383, G Loss: 1.5055\n",
      "Epoch [6/25], Step [700/938], D Loss: 0.8457, G Loss: 1.6424\n",
      "Epoch [6/25], Step [800/938], D Loss: 0.7066, G Loss: 1.6467\n",
      "Epoch [6/25], Step [900/938], D Loss: 0.9050, G Loss: 1.1563\n",
      "Epoch [7/25], Step [0/938], D Loss: 0.6905, G Loss: 1.7089\n",
      "Epoch [7/25], Step [100/938], D Loss: 0.6469, G Loss: 1.5907\n",
      "Epoch [7/25], Step [200/938], D Loss: 0.6806, G Loss: 1.9187\n",
      "Epoch [7/25], Step [300/938], D Loss: 0.7374, G Loss: 2.2196\n",
      "Epoch [7/25], Step [400/938], D Loss: 0.7419, G Loss: 1.1201\n",
      "Epoch [7/25], Step [500/938], D Loss: 0.6277, G Loss: 1.6795\n",
      "Epoch [7/25], Step [600/938], D Loss: 0.7313, G Loss: 1.8350\n",
      "Epoch [7/25], Step [700/938], D Loss: 0.9164, G Loss: 1.0378\n",
      "Epoch [7/25], Step [800/938], D Loss: 0.8047, G Loss: 1.1397\n",
      "Epoch [7/25], Step [900/938], D Loss: 0.7443, G Loss: 1.5309\n",
      "Epoch [8/25], Step [0/938], D Loss: 0.7097, G Loss: 1.3426\n",
      "Epoch [8/25], Step [100/938], D Loss: 0.8855, G Loss: 1.2271\n",
      "Epoch [8/25], Step [200/938], D Loss: 0.9363, G Loss: 1.3140\n",
      "Epoch [8/25], Step [300/938], D Loss: 1.0379, G Loss: 1.1708\n",
      "Epoch [8/25], Step [400/938], D Loss: 0.7941, G Loss: 1.5543\n",
      "Epoch [8/25], Step [500/938], D Loss: 0.7496, G Loss: 1.8420\n",
      "Epoch [8/25], Step [600/938], D Loss: 0.8475, G Loss: 1.3195\n",
      "Epoch [8/25], Step [700/938], D Loss: 0.7097, G Loss: 2.0844\n",
      "Epoch [8/25], Step [800/938], D Loss: 0.8706, G Loss: 1.4114\n",
      "Epoch [8/25], Step [900/938], D Loss: 0.8511, G Loss: 1.7570\n",
      "Epoch [9/25], Step [0/938], D Loss: 0.8638, G Loss: 1.5996\n",
      "Epoch [9/25], Step [100/938], D Loss: 0.8824, G Loss: 1.9042\n",
      "Epoch [9/25], Step [200/938], D Loss: 0.6781, G Loss: 1.7055\n",
      "Epoch [9/25], Step [300/938], D Loss: 0.7543, G Loss: 1.1412\n",
      "Epoch [9/25], Step [400/938], D Loss: 0.8277, G Loss: 1.1774\n",
      "Epoch [9/25], Step [500/938], D Loss: 1.0185, G Loss: 1.9401\n",
      "Epoch [9/25], Step [600/938], D Loss: 0.6709, G Loss: 2.1594\n",
      "Epoch [9/25], Step [700/938], D Loss: 0.9078, G Loss: 1.2504\n",
      "Epoch [9/25], Step [800/938], D Loss: 0.8701, G Loss: 2.1401\n",
      "Epoch [9/25], Step [900/938], D Loss: 0.8515, G Loss: 1.2628\n",
      "Epoch [10/25], Step [0/938], D Loss: 0.8044, G Loss: 1.3937\n",
      "Epoch [10/25], Step [100/938], D Loss: 0.6809, G Loss: 1.5727\n",
      "Epoch [10/25], Step [200/938], D Loss: 0.6928, G Loss: 1.2077\n",
      "Epoch [10/25], Step [300/938], D Loss: 0.8922, G Loss: 2.0966\n",
      "Epoch [10/25], Step [400/938], D Loss: 0.9124, G Loss: 1.6306\n",
      "Epoch [10/25], Step [500/938], D Loss: 0.6429, G Loss: 1.9250\n",
      "Epoch [10/25], Step [600/938], D Loss: 0.7602, G Loss: 1.5043\n",
      "Epoch [10/25], Step [700/938], D Loss: 0.9166, G Loss: 1.0849\n",
      "Epoch [10/25], Step [800/938], D Loss: 1.1473, G Loss: 1.4762\n",
      "Epoch [10/25], Step [900/938], D Loss: 0.7829, G Loss: 1.8028\n",
      "Epoch [11/25], Step [0/938], D Loss: 0.5396, G Loss: 1.8105\n",
      "Epoch [11/25], Step [100/938], D Loss: 0.6257, G Loss: 1.8405\n",
      "Epoch [11/25], Step [200/938], D Loss: 0.6300, G Loss: 2.1347\n",
      "Epoch [11/25], Step [300/938], D Loss: 0.5282, G Loss: 1.6902\n",
      "Epoch [11/25], Step [400/938], D Loss: 0.5123, G Loss: 2.1688\n",
      "Epoch [11/25], Step [500/938], D Loss: 0.6755, G Loss: 2.3912\n",
      "Epoch [11/25], Step [600/938], D Loss: 0.6851, G Loss: 1.7552\n",
      "Epoch [11/25], Step [700/938], D Loss: 0.6442, G Loss: 2.1750\n",
      "Epoch [11/25], Step [800/938], D Loss: 0.7319, G Loss: 1.8025\n",
      "Epoch [11/25], Step [900/938], D Loss: 0.9527, G Loss: 1.7075\n",
      "Epoch [12/25], Step [0/938], D Loss: 0.5533, G Loss: 2.0628\n",
      "Epoch [12/25], Step [100/938], D Loss: 0.7896, G Loss: 1.6886\n",
      "Epoch [12/25], Step [200/938], D Loss: 0.9483, G Loss: 1.1327\n",
      "Epoch [12/25], Step [300/938], D Loss: 0.8955, G Loss: 2.4317\n",
      "Epoch [12/25], Step [400/938], D Loss: 0.6759, G Loss: 1.9382\n",
      "Epoch [12/25], Step [500/938], D Loss: 0.6694, G Loss: 2.0645\n",
      "Epoch [12/25], Step [600/938], D Loss: 0.6535, G Loss: 1.9249\n",
      "Epoch [12/25], Step [700/938], D Loss: 0.9876, G Loss: 1.9109\n",
      "Epoch [12/25], Step [800/938], D Loss: 0.8028, G Loss: 2.7168\n",
      "Epoch [12/25], Step [900/938], D Loss: 0.6152, G Loss: 1.8902\n",
      "Epoch [13/25], Step [0/938], D Loss: 0.6304, G Loss: 2.0259\n",
      "Epoch [13/25], Step [100/938], D Loss: 0.7798, G Loss: 1.7947\n",
      "Epoch [13/25], Step [200/938], D Loss: 0.7203, G Loss: 1.5060\n",
      "Epoch [13/25], Step [300/938], D Loss: 0.7793, G Loss: 2.3393\n",
      "Epoch [13/25], Step [400/938], D Loss: 0.4464, G Loss: 1.9451\n",
      "Epoch [13/25], Step [500/938], D Loss: 0.4929, G Loss: 1.8564\n",
      "Epoch [13/25], Step [600/938], D Loss: 0.6275, G Loss: 1.8215\n",
      "Epoch [13/25], Step [700/938], D Loss: 0.7166, G Loss: 1.4728\n",
      "Epoch [13/25], Step [800/938], D Loss: 0.9309, G Loss: 1.6562\n",
      "Epoch [13/25], Step [900/938], D Loss: 0.7365, G Loss: 2.0247\n",
      "Epoch [14/25], Step [0/938], D Loss: 0.6652, G Loss: 1.5704\n",
      "Epoch [14/25], Step [100/938], D Loss: 0.7933, G Loss: 1.8991\n",
      "Epoch [14/25], Step [200/938], D Loss: 0.6634, G Loss: 2.0487\n",
      "Epoch [14/25], Step [300/938], D Loss: 0.6406, G Loss: 1.7378\n",
      "Epoch [14/25], Step [400/938], D Loss: 0.6220, G Loss: 2.3184\n",
      "Epoch [14/25], Step [500/938], D Loss: 0.7159, G Loss: 1.7849\n",
      "Epoch [14/25], Step [600/938], D Loss: 0.5388, G Loss: 2.2208\n",
      "Epoch [14/25], Step [700/938], D Loss: 1.0938, G Loss: 1.7537\n",
      "Epoch [14/25], Step [800/938], D Loss: 0.7303, G Loss: 2.0600\n",
      "Epoch [14/25], Step [900/938], D Loss: 0.7407, G Loss: 2.0509\n",
      "Epoch [15/25], Step [0/938], D Loss: 0.7572, G Loss: 2.4823\n",
      "Epoch [15/25], Step [100/938], D Loss: 0.9135, G Loss: 2.1925\n",
      "Epoch [15/25], Step [200/938], D Loss: 0.6806, G Loss: 2.0418\n",
      "Epoch [15/25], Step [300/938], D Loss: 0.6948, G Loss: 1.5255\n",
      "Epoch [15/25], Step [400/938], D Loss: 0.6997, G Loss: 1.4943\n",
      "Epoch [15/25], Step [500/938], D Loss: 0.8097, G Loss: 2.1994\n",
      "Epoch [15/25], Step [600/938], D Loss: 0.7063, G Loss: 1.7554\n",
      "Epoch [15/25], Step [700/938], D Loss: 0.9027, G Loss: 1.2773\n",
      "Epoch [15/25], Step [800/938], D Loss: 0.6709, G Loss: 1.8000\n",
      "Epoch [15/25], Step [900/938], D Loss: 0.8057, G Loss: 1.5432\n",
      "Epoch [16/25], Step [0/938], D Loss: 0.9584, G Loss: 1.5683\n",
      "Epoch [16/25], Step [100/938], D Loss: 0.7346, G Loss: 1.9264\n",
      "Epoch [16/25], Step [200/938], D Loss: 0.8476, G Loss: 1.3444\n",
      "Epoch [16/25], Step [300/938], D Loss: 0.7535, G Loss: 1.6252\n",
      "Epoch [16/25], Step [400/938], D Loss: 0.7444, G Loss: 2.0706\n",
      "Epoch [16/25], Step [500/938], D Loss: 0.6330, G Loss: 1.4919\n",
      "Epoch [16/25], Step [600/938], D Loss: 1.1398, G Loss: 1.7036\n",
      "Epoch [16/25], Step [700/938], D Loss: 0.7591, G Loss: 1.7672\n",
      "Epoch [16/25], Step [800/938], D Loss: 0.8482, G Loss: 1.3141\n",
      "Epoch [16/25], Step [900/938], D Loss: 1.0129, G Loss: 2.6906\n",
      "Epoch [17/25], Step [0/938], D Loss: 0.8913, G Loss: 1.7084\n",
      "Epoch [17/25], Step [100/938], D Loss: 1.2945, G Loss: 1.9997\n",
      "Epoch [17/25], Step [200/938], D Loss: 0.7839, G Loss: 1.3971\n",
      "Epoch [17/25], Step [300/938], D Loss: 0.9635, G Loss: 1.8595\n",
      "Epoch [17/25], Step [400/938], D Loss: 0.9385, G Loss: 1.5574\n",
      "Epoch [17/25], Step [500/938], D Loss: 0.8038, G Loss: 1.6596\n",
      "Epoch [17/25], Step [600/938], D Loss: 0.9022, G Loss: 1.3606\n",
      "Epoch [17/25], Step [700/938], D Loss: 0.9915, G Loss: 1.3596\n",
      "Epoch [17/25], Step [800/938], D Loss: 0.7618, G Loss: 1.2815\n",
      "Epoch [17/25], Step [900/938], D Loss: 1.1391, G Loss: 1.0301\n",
      "Epoch [18/25], Step [0/938], D Loss: 0.8765, G Loss: 1.1313\n",
      "Epoch [18/25], Step [100/938], D Loss: 0.7629, G Loss: 1.8429\n",
      "Epoch [18/25], Step [200/938], D Loss: 0.7376, G Loss: 1.6227\n",
      "Epoch [18/25], Step [300/938], D Loss: 1.1030, G Loss: 2.2573\n",
      "Epoch [18/25], Step [400/938], D Loss: 0.9820, G Loss: 1.6583\n",
      "Epoch [18/25], Step [500/938], D Loss: 0.8709, G Loss: 1.7200\n",
      "Epoch [18/25], Step [600/938], D Loss: 0.6973, G Loss: 1.8156\n",
      "Epoch [18/25], Step [700/938], D Loss: 1.0035, G Loss: 1.1001\n",
      "Epoch [18/25], Step [800/938], D Loss: 0.9634, G Loss: 1.5770\n",
      "Epoch [18/25], Step [900/938], D Loss: 0.9191, G Loss: 1.5582\n",
      "Epoch [19/25], Step [0/938], D Loss: 1.0609, G Loss: 2.1466\n",
      "Epoch [19/25], Step [100/938], D Loss: 0.8334, G Loss: 1.0317\n",
      "Epoch [19/25], Step [200/938], D Loss: 0.9368, G Loss: 2.0215\n",
      "Epoch [19/25], Step [300/938], D Loss: 0.9445, G Loss: 1.1632\n",
      "Epoch [19/25], Step [400/938], D Loss: 0.9676, G Loss: 1.2297\n",
      "Epoch [19/25], Step [500/938], D Loss: 1.2069, G Loss: 1.8314\n",
      "Epoch [19/25], Step [600/938], D Loss: 0.9669, G Loss: 1.4930\n",
      "Epoch [19/25], Step [700/938], D Loss: 0.9111, G Loss: 1.3161\n",
      "Epoch [19/25], Step [800/938], D Loss: 0.8312, G Loss: 1.3871\n",
      "Epoch [19/25], Step [900/938], D Loss: 0.9136, G Loss: 1.1297\n",
      "Epoch [20/25], Step [0/938], D Loss: 1.0781, G Loss: 1.7008\n",
      "Epoch [20/25], Step [100/938], D Loss: 0.6789, G Loss: 1.5211\n",
      "Epoch [20/25], Step [200/938], D Loss: 1.0084, G Loss: 1.5668\n",
      "Epoch [20/25], Step [300/938], D Loss: 0.8703, G Loss: 1.8827\n",
      "Epoch [20/25], Step [400/938], D Loss: 1.0670, G Loss: 1.4028\n",
      "Epoch [20/25], Step [500/938], D Loss: 0.8688, G Loss: 1.2660\n",
      "Epoch [20/25], Step [600/938], D Loss: 0.8204, G Loss: 1.7526\n",
      "Epoch [20/25], Step [700/938], D Loss: 0.9326, G Loss: 1.1139\n",
      "Epoch [20/25], Step [800/938], D Loss: 0.9413, G Loss: 1.2910\n",
      "Epoch [20/25], Step [900/938], D Loss: 1.0563, G Loss: 1.3397\n",
      "Epoch [21/25], Step [0/938], D Loss: 1.2142, G Loss: 1.7424\n",
      "Epoch [21/25], Step [100/938], D Loss: 1.2093, G Loss: 0.8982\n",
      "Epoch [21/25], Step [200/938], D Loss: 1.1047, G Loss: 1.4375\n",
      "Epoch [21/25], Step [300/938], D Loss: 1.1539, G Loss: 1.1056\n",
      "Epoch [21/25], Step [400/938], D Loss: 0.9094, G Loss: 1.7188\n",
      "Epoch [21/25], Step [500/938], D Loss: 1.0548, G Loss: 1.3974\n",
      "Epoch [21/25], Step [600/938], D Loss: 0.9530, G Loss: 1.4388\n",
      "Epoch [21/25], Step [700/938], D Loss: 0.9211, G Loss: 1.6322\n",
      "Epoch [21/25], Step [800/938], D Loss: 1.0558, G Loss: 1.7697\n",
      "Epoch [21/25], Step [900/938], D Loss: 1.0876, G Loss: 1.8013\n",
      "Epoch [22/25], Step [0/938], D Loss: 1.0027, G Loss: 0.9970\n",
      "Epoch [22/25], Step [100/938], D Loss: 1.1263, G Loss: 1.3188\n",
      "Epoch [22/25], Step [200/938], D Loss: 1.1286, G Loss: 1.2590\n",
      "Epoch [22/25], Step [300/938], D Loss: 1.2688, G Loss: 1.3907\n",
      "Epoch [22/25], Step [400/938], D Loss: 1.1405, G Loss: 1.1791\n",
      "Epoch [22/25], Step [500/938], D Loss: 1.1194, G Loss: 1.5969\n",
      "Epoch [22/25], Step [600/938], D Loss: 0.8890, G Loss: 0.9960\n",
      "Epoch [22/25], Step [700/938], D Loss: 0.9420, G Loss: 1.4549\n",
      "Epoch [22/25], Step [800/938], D Loss: 0.8986, G Loss: 1.3413\n",
      "Epoch [22/25], Step [900/938], D Loss: 0.9737, G Loss: 1.6265\n",
      "Epoch [23/25], Step [0/938], D Loss: 0.9606, G Loss: 1.4698\n",
      "Epoch [23/25], Step [100/938], D Loss: 1.0486, G Loss: 0.9183\n",
      "Epoch [23/25], Step [200/938], D Loss: 0.9182, G Loss: 1.5782\n",
      "Epoch [23/25], Step [300/938], D Loss: 0.9287, G Loss: 2.1022\n",
      "Epoch [23/25], Step [400/938], D Loss: 1.0025, G Loss: 1.8089\n",
      "Epoch [23/25], Step [500/938], D Loss: 1.1171, G Loss: 1.5191\n",
      "Epoch [23/25], Step [600/938], D Loss: 0.9149, G Loss: 1.5621\n",
      "Epoch [23/25], Step [700/938], D Loss: 1.0088, G Loss: 1.6183\n",
      "Epoch [23/25], Step [800/938], D Loss: 0.9322, G Loss: 1.2931\n",
      "Epoch [23/25], Step [900/938], D Loss: 0.9427, G Loss: 1.5510\n",
      "Epoch [24/25], Step [0/938], D Loss: 1.2852, G Loss: 0.6840\n",
      "Epoch [24/25], Step [100/938], D Loss: 0.9741, G Loss: 1.0880\n",
      "Epoch [24/25], Step [200/938], D Loss: 0.9716, G Loss: 1.3281\n",
      "Epoch [24/25], Step [300/938], D Loss: 0.7729, G Loss: 1.4550\n",
      "Epoch [24/25], Step [400/938], D Loss: 1.2146, G Loss: 0.9319\n",
      "Epoch [24/25], Step [500/938], D Loss: 0.8981, G Loss: 1.0368\n",
      "Epoch [24/25], Step [600/938], D Loss: 1.0053, G Loss: 2.0106\n",
      "Epoch [24/25], Step [700/938], D Loss: 0.8699, G Loss: 1.6016\n",
      "Epoch [24/25], Step [800/938], D Loss: 1.0085, G Loss: 1.8977\n",
      "Epoch [24/25], Step [900/938], D Loss: 1.1125, G Loss: 1.4272\n",
      "Epoch [25/25], Step [0/938], D Loss: 1.0592, G Loss: 1.1428\n",
      "Epoch [25/25], Step [100/938], D Loss: 0.9192, G Loss: 0.9537\n",
      "Epoch [25/25], Step [200/938], D Loss: 1.0490, G Loss: 0.9712\n",
      "Epoch [25/25], Step [300/938], D Loss: 1.1112, G Loss: 1.1829\n",
      "Epoch [25/25], Step [400/938], D Loss: 1.0142, G Loss: 1.3504\n",
      "Epoch [25/25], Step [500/938], D Loss: 1.0073, G Loss: 1.4669\n",
      "Epoch [25/25], Step [600/938], D Loss: 0.9104, G Loss: 1.7280\n",
      "Epoch [25/25], Step [700/938], D Loss: 1.2247, G Loss: 1.4601\n",
      "Epoch [25/25], Step [800/938], D Loss: 1.0947, G Loss: 1.3235\n",
      "Epoch [25/25], Step [900/938], D Loss: 0.9880, G Loss: 1.0482\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "        \n",
    "        ### Train Discriminator ##\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        ### Train Generator ##\n",
    "        generator_optimizer.zero_grad()\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)  # Fool the discriminator!\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb5fa4-1113-470e-b2f1-6abf09745c41",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
