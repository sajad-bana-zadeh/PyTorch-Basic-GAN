{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72241b46-34c8-4d2d-a682-6f0b0372a7da",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c65043f-82f2-4115-abec-6d4bf48e6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5370775-9674-493c-9369-85a2abd835e4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7b516-dab4-4d00-90a5-9df7d9dcfd01",
   "metadata": {},
   "source": [
    "## 2. Prepare the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ee324-45d5-466e-a8b7-7bdb0e4f0098",
   "metadata": {},
   "source": [
    "- Use MNIST as a typical example, normalize it to [-1, 1] for better GAN performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36a4d3-08a3-448f-b4bd-926a557b4640",
   "metadata": {},
   "source": [
    "This command creates a **data transformation pipeline** typically used in image preprocessing for deep learning tasks with libraries like PyTorch.\n",
    "\n",
    "Here’s what the command does, step-by-step:\n",
    "\n",
    "1. **`transforms.Compose([...])`**  \n",
    "   This wraps multiple transformations into one. It applies them sequentially to the input data.\n",
    "\n",
    "2. **`transforms.ToTensor()`**  \n",
    "   Converts a PIL image or NumPy ndarray into a PyTorch tensor. It also scales pixel values from the range  to [0.0, 1.0].\n",
    "\n",
    "3. **`transforms.Normalize(mean=(0.5,), std=(0.5,))`**  \n",
    "   Normalizes the tensor by subtracting the mean and dividing by the standard deviation for each channel. Here, the single value `(0.5,)` corresponds to one channel (like grayscale).\n",
    "\n",
    "   Since the pixel values are between 0 and 1 after `ToTensor()`, normalization with mean=0.5 and std=0.5 converts them roughly to the range [-1, 1] using this formula:\n",
    "\n",
    "   $$\n",
    "   \\text{normalized\\_pixel} = \\frac{\\text{pixel} - 0.5}{0.5}\n",
    "   $$\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- **Convert images to tensors** (making them compatible with PyTorch models).\n",
    "- **Normalize pixel intensity values** so the model trains more effectively (helps with convergence and stabilizes training by standardizing input).\n",
    "\n",
    "This transformation is common in training neural networks on datasets like MNIST or grayscale images since it processes the image into a normalized tensor input suitable for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce9e330-4d34-4f5f-a08b-c772756154f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dec174-af00-49c9-a7e4-59bfd3b388c2",
   "metadata": {},
   "source": [
    "This command creates a **training dataset object** for the MNIST dataset using PyTorch’s `torchvision.datasets` module.\n",
    "\n",
    "Here’s a breakdown of the command:\n",
    "\n",
    "- **`datasets.MNIST(...)`**  \n",
    "  This loads the MNIST handwritten digit dataset, which contains grayscale images of digits (0–9).\n",
    "\n",
    "- **`root='./data'`**  \n",
    "  Specifies the directory where the MNIST data will be stored or loaded from. If the data is not already present, it will be downloaded to this folder.\n",
    "\n",
    "- **`train=True`**  \n",
    "  Indicates that this dataset object should load the **training split** of MNIST (60,000 images). If set to `False`, it would load the test split (10,000 images).\n",
    "\n",
    "- **`download=True`**  \n",
    "  If the dataset isn’t already present in the specified `root` folder, PyTorch will automatically download it from the internet.\n",
    "\n",
    "- **`transform=transform`**  \n",
    "  Applies the previously defined transformations (such as conversion to tensor and normalization) to every image when it is loaded. This preprocesses the images so they're ready for training.\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- It **loads** the MNIST training dataset, downloading it if necessary.\n",
    "- It **applies the defined transformations** (like converting images to tensors and normalizing) on the data.\n",
    "- It prepares the dataset in a format suitable for training a PyTorch model.\n",
    "\n",
    "In short, this command **sets up your training data pipeline** for feeding images into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c37f00c-b8eb-4dd3-89c4-f3e0549ffe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861b99f-1c6b-48e2-9614-d13229ab29b0",
   "metadata": {},
   "source": [
    "This command creates a **DataLoader** object from the previously defined `train_dataset` (MNIST training dataset in this case).\n",
    "\n",
    "Here’s what it does:\n",
    "\n",
    "- **`DataLoader(train_dataset, batch_size=64, shuffle=True)`**  \n",
    "  This wraps the dataset in a convenient iterator for training machine learning models.\n",
    "\n",
    "- **`batch_size=64`**  \n",
    "  Specifies that data will be loaded in batches of 64 samples. Instead of passing one image at a time, the model receives 64 images in each training step, which is efficient and stabilizes training.\n",
    "\n",
    "- **`shuffle=True`**  \n",
    "  Means the dataset will be shuffled at the start of each epoch. Shuffling helps the model see the data in a different order each time, which improves training by reducing overfitting and increasing generalization.\n",
    "\n",
    "### Purpose of this command:\n",
    "\n",
    "- To **batch the dataset** enabling efficient processing and training.\n",
    "- To **shuffle the training data** so the model does not learn the order of data, which promotes better generalization.\n",
    "- To provide an **easily iterable object** that returns batches of transformed images and their labels, ready for training.\n",
    "\n",
    "In summary, this command sets up the training pipeline to feed the model mini-batches of shuffled data for training. It abstracts away loading and batching complexities so the training loop can run smoothly and efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4633ff2d-baaa-423f-ac15-203133daf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01e6fc8-79e5-49a7-819d-dfea2305add4",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e90aef-9dcf-4771-a746-3cddb2aeef12",
   "metadata": {},
   "source": [
    "## 3. Define the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ccdc1-37d4-4995-a7df-8723362df7ff",
   "metadata": {},
   "source": [
    "- **Generator:** Turns noisy random input into a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e54d409-c5dc-4216-8b77-fbe446b48c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Tanh()  # Output between -1 and 1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830491e8-95b4-402a-9fec-3b650252390d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbc295-2c83-4520-b80b-cf12ddc17295",
   "metadata": {},
   "source": [
    "## 4. Define the Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840e5da-95c4-49de-a52e-d4a11bed1407",
   "metadata": {},
   "source": [
    "- **Discriminator:** Predicts if input image is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec1d4f7e-6d7a-46c7-a4de-e836a765a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4681a0-ee7b-4b86-9601-8adb38e2e663",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb03909a-5eae-4a03-95d8-7da29473587e",
   "metadata": {},
   "source": [
    "## 5. Initialize networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fe443f-84ce-433a-a5c4-2f6692ea3a0b",
   "metadata": {},
   "source": [
    "- Set your device (GPU if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a07d63f5-cd9c-470f-a0e0-c19b44660adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4736ed-9685-4538-a285-6a28827e9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "generator = Generator(noise_dim).to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0cbff4-ab6f-477a-a416-e0cc17442e4d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8fbf88-1b5e-4c9a-91e4-51d6b0c399af",
   "metadata": {},
   "source": [
    "## 6. Set Loss and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8caf94d6-a5f4-4572-bcd8-9dddb9abc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9138f80c-c0ab-4cb8-b911-07c0d8a251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea54115-7ea5-41b1-bcbc-79ee4e68a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b57d33-7464-493d-8f66-63870626967c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5bfca-e65f-406d-81b9-67e6e958270c",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3e11b-b001-45ce-b1f0-b3066be0f6ee",
   "metadata": {},
   "source": [
    "For each batch:\n",
    "- Train D: on real images (label=1) and fake images (label=0).\n",
    "- Train G: try to make D classify generated images as real (label=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "967f2a14-7ad7-49a1-bb21-ba8c77103d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee96778e-d408-4b2c-a166-edf6ed4b4347",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/938], D Loss: 1.5021, G Loss: 0.6811\n",
      "Epoch [1/100], Step [100/938], D Loss: 1.0657, G Loss: 0.5572\n",
      "Epoch [1/100], Step [200/938], D Loss: 1.1630, G Loss: 0.5739\n",
      "Epoch [1/100], Step [300/938], D Loss: 1.1649, G Loss: 0.6321\n",
      "Epoch [1/100], Step [400/938], D Loss: 1.0775, G Loss: 0.7812\n",
      "Epoch [1/100], Step [500/938], D Loss: 0.9454, G Loss: 0.8511\n",
      "Epoch [1/100], Step [600/938], D Loss: 0.8461, G Loss: 1.0556\n",
      "Epoch [1/100], Step [700/938], D Loss: 0.8821, G Loss: 1.0138\n",
      "Epoch [1/100], Step [800/938], D Loss: 0.9163, G Loss: 1.0267\n",
      "Epoch [1/100], Step [900/938], D Loss: 0.9131, G Loss: 0.9326\n",
      "Epoch [2/100], Step [0/938], D Loss: 0.8923, G Loss: 1.1721\n",
      "Epoch [2/100], Step [100/938], D Loss: 0.9638, G Loss: 0.8203\n",
      "Epoch [2/100], Step [200/938], D Loss: 0.8644, G Loss: 1.0128\n",
      "Epoch [2/100], Step [300/938], D Loss: 0.7991, G Loss: 1.2784\n",
      "Epoch [2/100], Step [400/938], D Loss: 0.8533, G Loss: 1.0733\n",
      "Epoch [2/100], Step [500/938], D Loss: 0.8369, G Loss: 1.1190\n",
      "Epoch [2/100], Step [600/938], D Loss: 0.8617, G Loss: 1.4050\n",
      "Epoch [2/100], Step [700/938], D Loss: 0.7893, G Loss: 1.1879\n",
      "Epoch [2/100], Step [800/938], D Loss: 0.8623, G Loss: 1.3401\n",
      "Epoch [2/100], Step [900/938], D Loss: 0.8033, G Loss: 1.4581\n",
      "Epoch [3/100], Step [0/938], D Loss: 0.8310, G Loss: 0.9843\n",
      "Epoch [3/100], Step [100/938], D Loss: 0.7886, G Loss: 1.0846\n",
      "Epoch [3/100], Step [200/938], D Loss: 0.8329, G Loss: 1.4295\n",
      "Epoch [3/100], Step [300/938], D Loss: 0.7747, G Loss: 0.9720\n",
      "Epoch [3/100], Step [400/938], D Loss: 0.7318, G Loss: 1.3679\n",
      "Epoch [3/100], Step [500/938], D Loss: 0.8205, G Loss: 1.3627\n",
      "Epoch [3/100], Step [600/938], D Loss: 0.7728, G Loss: 1.4463\n",
      "Epoch [3/100], Step [700/938], D Loss: 0.7261, G Loss: 1.2490\n",
      "Epoch [3/100], Step [800/938], D Loss: 0.7881, G Loss: 1.5957\n",
      "Epoch [3/100], Step [900/938], D Loss: 0.8381, G Loss: 1.6950\n",
      "Epoch [4/100], Step [0/938], D Loss: 0.7269, G Loss: 1.1856\n",
      "Epoch [4/100], Step [100/938], D Loss: 0.9213, G Loss: 1.8996\n",
      "Epoch [4/100], Step [200/938], D Loss: 0.8049, G Loss: 1.4400\n",
      "Epoch [4/100], Step [300/938], D Loss: 0.7632, G Loss: 1.5482\n",
      "Epoch [4/100], Step [400/938], D Loss: 0.6679, G Loss: 1.3153\n",
      "Epoch [4/100], Step [500/938], D Loss: 0.7577, G Loss: 1.4364\n",
      "Epoch [4/100], Step [600/938], D Loss: 0.7101, G Loss: 1.2893\n",
      "Epoch [4/100], Step [700/938], D Loss: 0.6159, G Loss: 1.3571\n",
      "Epoch [4/100], Step [800/938], D Loss: 0.8046, G Loss: 1.7424\n",
      "Epoch [4/100], Step [900/938], D Loss: 0.7578, G Loss: 1.3400\n",
      "Epoch [5/100], Step [0/938], D Loss: 1.0372, G Loss: 2.1923\n",
      "Epoch [5/100], Step [100/938], D Loss: 0.7854, G Loss: 1.1343\n",
      "Epoch [5/100], Step [200/938], D Loss: 0.6402, G Loss: 1.5156\n",
      "Epoch [5/100], Step [300/938], D Loss: 0.6890, G Loss: 2.2388\n",
      "Epoch [5/100], Step [400/938], D Loss: 0.5761, G Loss: 1.9364\n",
      "Epoch [5/100], Step [500/938], D Loss: 0.6890, G Loss: 2.1404\n",
      "Epoch [5/100], Step [600/938], D Loss: 0.7654, G Loss: 2.1315\n",
      "Epoch [5/100], Step [700/938], D Loss: 0.6679, G Loss: 1.5691\n",
      "Epoch [5/100], Step [800/938], D Loss: 0.7888, G Loss: 1.2897\n",
      "Epoch [5/100], Step [900/938], D Loss: 0.8090, G Loss: 1.6671\n",
      "Epoch [6/100], Step [0/938], D Loss: 0.7566, G Loss: 1.5416\n",
      "Epoch [6/100], Step [100/938], D Loss: 0.9851, G Loss: 1.0245\n",
      "Epoch [6/100], Step [200/938], D Loss: 0.8312, G Loss: 1.2721\n",
      "Epoch [6/100], Step [300/938], D Loss: 0.8694, G Loss: 1.3067\n",
      "Epoch [6/100], Step [400/938], D Loss: 0.7352, G Loss: 1.7095\n",
      "Epoch [6/100], Step [500/938], D Loss: 0.8238, G Loss: 1.6191\n",
      "Epoch [6/100], Step [600/938], D Loss: 0.7571, G Loss: 1.5694\n",
      "Epoch [6/100], Step [700/938], D Loss: 0.7145, G Loss: 1.5812\n",
      "Epoch [6/100], Step [800/938], D Loss: 0.8053, G Loss: 1.4725\n",
      "Epoch [6/100], Step [900/938], D Loss: 0.7345, G Loss: 1.0859\n",
      "Epoch [7/100], Step [0/938], D Loss: 0.8225, G Loss: 1.4238\n",
      "Epoch [7/100], Step [100/938], D Loss: 0.7813, G Loss: 1.4564\n",
      "Epoch [7/100], Step [200/938], D Loss: 0.9930, G Loss: 0.9926\n",
      "Epoch [7/100], Step [300/938], D Loss: 0.8531, G Loss: 1.3192\n",
      "Epoch [7/100], Step [400/938], D Loss: 0.6574, G Loss: 1.6384\n",
      "Epoch [7/100], Step [500/938], D Loss: 0.8118, G Loss: 1.1587\n",
      "Epoch [7/100], Step [600/938], D Loss: 0.7713, G Loss: 1.7551\n",
      "Epoch [7/100], Step [700/938], D Loss: 0.8058, G Loss: 1.0650\n",
      "Epoch [7/100], Step [800/938], D Loss: 0.8803, G Loss: 1.2409\n",
      "Epoch [7/100], Step [900/938], D Loss: 0.9059, G Loss: 1.1505\n",
      "Epoch [8/100], Step [0/938], D Loss: 0.8016, G Loss: 1.4364\n",
      "Epoch [8/100], Step [100/938], D Loss: 0.8542, G Loss: 1.3259\n",
      "Epoch [8/100], Step [200/938], D Loss: 0.7885, G Loss: 1.5742\n",
      "Epoch [8/100], Step [300/938], D Loss: 0.9633, G Loss: 1.1755\n",
      "Epoch [8/100], Step [400/938], D Loss: 0.7410, G Loss: 1.4319\n",
      "Epoch [8/100], Step [500/938], D Loss: 0.7724, G Loss: 1.2324\n",
      "Epoch [8/100], Step [600/938], D Loss: 0.7136, G Loss: 1.8280\n",
      "Epoch [8/100], Step [700/938], D Loss: 0.7961, G Loss: 1.3221\n",
      "Epoch [8/100], Step [800/938], D Loss: 0.8136, G Loss: 1.3122\n",
      "Epoch [8/100], Step [900/938], D Loss: 0.8442, G Loss: 1.7782\n",
      "Epoch [9/100], Step [0/938], D Loss: 0.9257, G Loss: 1.2421\n",
      "Epoch [9/100], Step [100/938], D Loss: 0.7664, G Loss: 1.3126\n",
      "Epoch [9/100], Step [200/938], D Loss: 1.0203, G Loss: 1.8115\n",
      "Epoch [9/100], Step [300/938], D Loss: 1.0668, G Loss: 1.2391\n",
      "Epoch [9/100], Step [400/938], D Loss: 0.9589, G Loss: 1.2000\n",
      "Epoch [9/100], Step [500/938], D Loss: 0.9382, G Loss: 1.6179\n",
      "Epoch [9/100], Step [600/938], D Loss: 0.9181, G Loss: 1.5350\n",
      "Epoch [9/100], Step [700/938], D Loss: 0.9479, G Loss: 1.0499\n",
      "Epoch [9/100], Step [800/938], D Loss: 0.8435, G Loss: 1.5044\n",
      "Epoch [9/100], Step [900/938], D Loss: 1.0275, G Loss: 1.4678\n",
      "Epoch [10/100], Step [0/938], D Loss: 1.0295, G Loss: 1.1114\n",
      "Epoch [10/100], Step [100/938], D Loss: 0.8499, G Loss: 1.4687\n",
      "Epoch [10/100], Step [200/938], D Loss: 0.8998, G Loss: 0.9472\n",
      "Epoch [10/100], Step [300/938], D Loss: 0.7959, G Loss: 1.3043\n",
      "Epoch [10/100], Step [400/938], D Loss: 0.9358, G Loss: 0.8750\n",
      "Epoch [10/100], Step [500/938], D Loss: 0.6906, G Loss: 1.3486\n",
      "Epoch [10/100], Step [600/938], D Loss: 0.7738, G Loss: 1.1740\n",
      "Epoch [10/100], Step [700/938], D Loss: 0.8900, G Loss: 1.4407\n",
      "Epoch [10/100], Step [800/938], D Loss: 0.9988, G Loss: 1.1310\n",
      "Epoch [10/100], Step [900/938], D Loss: 0.8829, G Loss: 1.3243\n",
      "Epoch [11/100], Step [0/938], D Loss: 0.9785, G Loss: 0.9041\n",
      "Epoch [11/100], Step [100/938], D Loss: 0.8553, G Loss: 0.9548\n",
      "Epoch [11/100], Step [200/938], D Loss: 0.9247, G Loss: 1.9118\n",
      "Epoch [11/100], Step [300/938], D Loss: 0.9533, G Loss: 1.3390\n",
      "Epoch [11/100], Step [400/938], D Loss: 0.9470, G Loss: 1.0129\n",
      "Epoch [11/100], Step [500/938], D Loss: 1.0216, G Loss: 1.6545\n",
      "Epoch [11/100], Step [600/938], D Loss: 0.9343, G Loss: 1.8188\n",
      "Epoch [11/100], Step [700/938], D Loss: 0.6423, G Loss: 2.0442\n",
      "Epoch [11/100], Step [800/938], D Loss: 0.8746, G Loss: 2.0614\n",
      "Epoch [11/100], Step [900/938], D Loss: 0.7763, G Loss: 1.8014\n",
      "Epoch [12/100], Step [0/938], D Loss: 0.8115, G Loss: 1.3037\n",
      "Epoch [12/100], Step [100/938], D Loss: 0.8686, G Loss: 1.1187\n",
      "Epoch [12/100], Step [200/938], D Loss: 0.8939, G Loss: 1.4464\n",
      "Epoch [12/100], Step [300/938], D Loss: 0.7462, G Loss: 1.4432\n",
      "Epoch [12/100], Step [400/938], D Loss: 0.6576, G Loss: 2.2268\n",
      "Epoch [12/100], Step [500/938], D Loss: 0.7105, G Loss: 1.7714\n",
      "Epoch [12/100], Step [600/938], D Loss: 0.8877, G Loss: 1.2359\n",
      "Epoch [12/100], Step [700/938], D Loss: 0.7290, G Loss: 1.6236\n",
      "Epoch [12/100], Step [800/938], D Loss: 0.7115, G Loss: 1.2973\n",
      "Epoch [12/100], Step [900/938], D Loss: 1.0248, G Loss: 0.9938\n",
      "Epoch [13/100], Step [0/938], D Loss: 0.8749, G Loss: 1.0098\n",
      "Epoch [13/100], Step [100/938], D Loss: 0.6436, G Loss: 2.1528\n",
      "Epoch [13/100], Step [200/938], D Loss: 0.9119, G Loss: 1.1715\n",
      "Epoch [13/100], Step [300/938], D Loss: 0.7343, G Loss: 1.7260\n",
      "Epoch [13/100], Step [400/938], D Loss: 0.6753, G Loss: 1.7488\n",
      "Epoch [13/100], Step [500/938], D Loss: 0.7481, G Loss: 1.4138\n",
      "Epoch [13/100], Step [600/938], D Loss: 0.7447, G Loss: 1.2731\n",
      "Epoch [13/100], Step [700/938], D Loss: 0.5809, G Loss: 2.0077\n",
      "Epoch [13/100], Step [800/938], D Loss: 0.7156, G Loss: 1.9407\n",
      "Epoch [13/100], Step [900/938], D Loss: 0.6594, G Loss: 1.6536\n",
      "Epoch [14/100], Step [0/938], D Loss: 0.7614, G Loss: 1.9266\n",
      "Epoch [14/100], Step [100/938], D Loss: 0.7534, G Loss: 1.8315\n",
      "Epoch [14/100], Step [200/938], D Loss: 0.5778, G Loss: 1.9885\n",
      "Epoch [14/100], Step [300/938], D Loss: 0.8275, G Loss: 1.4941\n",
      "Epoch [14/100], Step [400/938], D Loss: 0.6099, G Loss: 2.0795\n",
      "Epoch [14/100], Step [500/938], D Loss: 0.6895, G Loss: 1.4373\n",
      "Epoch [14/100], Step [600/938], D Loss: 0.6986, G Loss: 2.2342\n",
      "Epoch [14/100], Step [700/938], D Loss: 0.8232, G Loss: 1.3819\n",
      "Epoch [14/100], Step [800/938], D Loss: 0.8415, G Loss: 1.5937\n",
      "Epoch [14/100], Step [900/938], D Loss: 0.6879, G Loss: 1.7201\n",
      "Epoch [15/100], Step [0/938], D Loss: 0.8032, G Loss: 1.3511\n",
      "Epoch [15/100], Step [100/938], D Loss: 0.8915, G Loss: 1.2550\n",
      "Epoch [15/100], Step [200/938], D Loss: 0.7380, G Loss: 1.3698\n",
      "Epoch [15/100], Step [300/938], D Loss: 0.8522, G Loss: 1.3406\n",
      "Epoch [15/100], Step [400/938], D Loss: 0.7784, G Loss: 1.3913\n",
      "Epoch [15/100], Step [500/938], D Loss: 1.1665, G Loss: 1.2286\n",
      "Epoch [15/100], Step [600/938], D Loss: 0.8109, G Loss: 1.4023\n",
      "Epoch [15/100], Step [700/938], D Loss: 0.9454, G Loss: 1.9251\n",
      "Epoch [15/100], Step [800/938], D Loss: 0.7671, G Loss: 1.5910\n",
      "Epoch [15/100], Step [900/938], D Loss: 0.8135, G Loss: 1.5013\n",
      "Epoch [16/100], Step [0/938], D Loss: 0.9583, G Loss: 1.2189\n",
      "Epoch [16/100], Step [100/938], D Loss: 0.7009, G Loss: 1.7379\n",
      "Epoch [16/100], Step [200/938], D Loss: 0.8885, G Loss: 1.2095\n",
      "Epoch [16/100], Step [300/938], D Loss: 0.9154, G Loss: 2.4128\n",
      "Epoch [16/100], Step [400/938], D Loss: 0.9496, G Loss: 1.2586\n",
      "Epoch [16/100], Step [500/938], D Loss: 0.8330, G Loss: 1.3320\n",
      "Epoch [16/100], Step [600/938], D Loss: 0.7501, G Loss: 2.1449\n",
      "Epoch [16/100], Step [700/938], D Loss: 0.7986, G Loss: 1.4611\n",
      "Epoch [16/100], Step [800/938], D Loss: 1.0291, G Loss: 1.0528\n",
      "Epoch [16/100], Step [900/938], D Loss: 0.8767, G Loss: 1.6206\n",
      "Epoch [17/100], Step [0/938], D Loss: 0.7302, G Loss: 1.3865\n",
      "Epoch [17/100], Step [100/938], D Loss: 0.9628, G Loss: 1.9475\n",
      "Epoch [17/100], Step [200/938], D Loss: 0.7597, G Loss: 2.4188\n",
      "Epoch [17/100], Step [300/938], D Loss: 0.8508, G Loss: 1.3887\n",
      "Epoch [17/100], Step [400/938], D Loss: 0.9921, G Loss: 1.3749\n",
      "Epoch [17/100], Step [500/938], D Loss: 0.8717, G Loss: 1.9641\n",
      "Epoch [17/100], Step [600/938], D Loss: 0.8652, G Loss: 1.8082\n",
      "Epoch [17/100], Step [700/938], D Loss: 0.8283, G Loss: 2.1470\n",
      "Epoch [17/100], Step [800/938], D Loss: 0.7857, G Loss: 1.5111\n",
      "Epoch [17/100], Step [900/938], D Loss: 0.9522, G Loss: 1.2412\n",
      "Epoch [18/100], Step [0/938], D Loss: 1.1155, G Loss: 1.2152\n",
      "Epoch [18/100], Step [100/938], D Loss: 0.8806, G Loss: 1.3944\n",
      "Epoch [18/100], Step [200/938], D Loss: 0.9180, G Loss: 1.8426\n",
      "Epoch [18/100], Step [300/938], D Loss: 0.8550, G Loss: 1.5315\n",
      "Epoch [18/100], Step [400/938], D Loss: 0.8926, G Loss: 1.5967\n",
      "Epoch [18/100], Step [500/938], D Loss: 0.8513, G Loss: 1.2050\n",
      "Epoch [18/100], Step [600/938], D Loss: 0.9569, G Loss: 1.3579\n",
      "Epoch [18/100], Step [700/938], D Loss: 0.8274, G Loss: 1.2537\n",
      "Epoch [18/100], Step [800/938], D Loss: 1.1056, G Loss: 0.9472\n",
      "Epoch [18/100], Step [900/938], D Loss: 1.0532, G Loss: 1.0168\n",
      "Epoch [19/100], Step [0/938], D Loss: 1.0600, G Loss: 1.4585\n",
      "Epoch [19/100], Step [100/938], D Loss: 1.1665, G Loss: 2.3049\n",
      "Epoch [19/100], Step [200/938], D Loss: 1.0994, G Loss: 0.8379\n",
      "Epoch [19/100], Step [300/938], D Loss: 0.8194, G Loss: 1.1912\n",
      "Epoch [19/100], Step [400/938], D Loss: 0.8768, G Loss: 1.3797\n",
      "Epoch [19/100], Step [500/938], D Loss: 0.8287, G Loss: 0.9936\n",
      "Epoch [19/100], Step [600/938], D Loss: 0.9834, G Loss: 1.1484\n",
      "Epoch [19/100], Step [700/938], D Loss: 1.0656, G Loss: 1.3545\n",
      "Epoch [19/100], Step [800/938], D Loss: 0.9939, G Loss: 1.3796\n",
      "Epoch [19/100], Step [900/938], D Loss: 0.9644, G Loss: 1.4309\n",
      "Epoch [20/100], Step [0/938], D Loss: 1.1005, G Loss: 1.3390\n",
      "Epoch [20/100], Step [100/938], D Loss: 0.9672, G Loss: 1.4382\n",
      "Epoch [20/100], Step [200/938], D Loss: 1.1147, G Loss: 0.8477\n",
      "Epoch [20/100], Step [300/938], D Loss: 0.8276, G Loss: 1.0926\n",
      "Epoch [20/100], Step [400/938], D Loss: 0.9270, G Loss: 1.0692\n",
      "Epoch [20/100], Step [500/938], D Loss: 1.0331, G Loss: 1.6191\n",
      "Epoch [20/100], Step [600/938], D Loss: 0.9803, G Loss: 0.9430\n",
      "Epoch [20/100], Step [700/938], D Loss: 1.1393, G Loss: 1.0992\n",
      "Epoch [20/100], Step [800/938], D Loss: 1.3190, G Loss: 0.6988\n",
      "Epoch [20/100], Step [900/938], D Loss: 0.8541, G Loss: 1.5233\n",
      "Epoch [21/100], Step [0/938], D Loss: 1.0447, G Loss: 0.8118\n",
      "Epoch [21/100], Step [100/938], D Loss: 0.9215, G Loss: 1.0259\n",
      "Epoch [21/100], Step [200/938], D Loss: 1.1400, G Loss: 1.6237\n",
      "Epoch [21/100], Step [300/938], D Loss: 1.0600, G Loss: 1.4322\n",
      "Epoch [21/100], Step [400/938], D Loss: 0.9117, G Loss: 1.7029\n",
      "Epoch [21/100], Step [500/938], D Loss: 1.0049, G Loss: 0.8506\n",
      "Epoch [21/100], Step [600/938], D Loss: 0.9906, G Loss: 1.1189\n",
      "Epoch [21/100], Step [700/938], D Loss: 1.0843, G Loss: 1.2836\n",
      "Epoch [21/100], Step [800/938], D Loss: 1.1727, G Loss: 2.3543\n",
      "Epoch [21/100], Step [900/938], D Loss: 1.0043, G Loss: 1.2600\n",
      "Epoch [22/100], Step [0/938], D Loss: 1.1760, G Loss: 0.8556\n",
      "Epoch [22/100], Step [100/938], D Loss: 1.1545, G Loss: 0.9977\n",
      "Epoch [22/100], Step [200/938], D Loss: 1.1063, G Loss: 0.9708\n",
      "Epoch [22/100], Step [300/938], D Loss: 0.8656, G Loss: 1.7386\n",
      "Epoch [22/100], Step [400/938], D Loss: 1.1936, G Loss: 1.1095\n",
      "Epoch [22/100], Step [500/938], D Loss: 1.1303, G Loss: 1.1107\n",
      "Epoch [22/100], Step [600/938], D Loss: 0.9868, G Loss: 1.4855\n",
      "Epoch [22/100], Step [700/938], D Loss: 1.0263, G Loss: 1.1856\n",
      "Epoch [22/100], Step [800/938], D Loss: 1.1979, G Loss: 1.1504\n",
      "Epoch [22/100], Step [900/938], D Loss: 0.9386, G Loss: 1.5907\n",
      "Epoch [23/100], Step [0/938], D Loss: 1.0999, G Loss: 2.0376\n",
      "Epoch [23/100], Step [100/938], D Loss: 1.2258, G Loss: 1.6592\n",
      "Epoch [23/100], Step [200/938], D Loss: 1.2465, G Loss: 0.9818\n",
      "Epoch [23/100], Step [300/938], D Loss: 1.2347, G Loss: 0.6568\n",
      "Epoch [23/100], Step [400/938], D Loss: 1.1995, G Loss: 0.8131\n",
      "Epoch [23/100], Step [500/938], D Loss: 1.0001, G Loss: 1.0931\n",
      "Epoch [23/100], Step [600/938], D Loss: 1.0362, G Loss: 1.6104\n",
      "Epoch [23/100], Step [700/938], D Loss: 1.1068, G Loss: 1.1541\n",
      "Epoch [23/100], Step [800/938], D Loss: 1.0909, G Loss: 1.0768\n",
      "Epoch [23/100], Step [900/938], D Loss: 1.0433, G Loss: 1.0691\n",
      "Epoch [24/100], Step [0/938], D Loss: 1.3896, G Loss: 0.5806\n",
      "Epoch [24/100], Step [100/938], D Loss: 1.0701, G Loss: 1.0671\n",
      "Epoch [24/100], Step [200/938], D Loss: 1.1954, G Loss: 0.9370\n",
      "Epoch [24/100], Step [300/938], D Loss: 1.0972, G Loss: 0.8530\n",
      "Epoch [24/100], Step [400/938], D Loss: 0.9782, G Loss: 1.2919\n",
      "Epoch [24/100], Step [500/938], D Loss: 1.1119, G Loss: 1.9403\n",
      "Epoch [24/100], Step [600/938], D Loss: 1.0254, G Loss: 0.8929\n",
      "Epoch [24/100], Step [700/938], D Loss: 0.9342, G Loss: 1.2095\n",
      "Epoch [24/100], Step [800/938], D Loss: 1.0856, G Loss: 1.8117\n",
      "Epoch [24/100], Step [900/938], D Loss: 1.0025, G Loss: 1.0910\n",
      "Epoch [25/100], Step [0/938], D Loss: 1.2034, G Loss: 0.9573\n",
      "Epoch [25/100], Step [100/938], D Loss: 1.0501, G Loss: 1.0443\n",
      "Epoch [25/100], Step [200/938], D Loss: 1.1178, G Loss: 1.1767\n",
      "Epoch [25/100], Step [300/938], D Loss: 1.1019, G Loss: 1.2894\n",
      "Epoch [25/100], Step [400/938], D Loss: 1.0809, G Loss: 1.2299\n",
      "Epoch [25/100], Step [500/938], D Loss: 1.0434, G Loss: 1.3157\n",
      "Epoch [25/100], Step [600/938], D Loss: 0.9061, G Loss: 1.3124\n",
      "Epoch [25/100], Step [700/938], D Loss: 1.0664, G Loss: 1.0254\n",
      "Epoch [25/100], Step [800/938], D Loss: 1.1521, G Loss: 1.0271\n",
      "Epoch [25/100], Step [900/938], D Loss: 1.0035, G Loss: 1.3046\n",
      "Epoch [26/100], Step [0/938], D Loss: 1.2166, G Loss: 1.1412\n",
      "Epoch [26/100], Step [100/938], D Loss: 1.1726, G Loss: 1.1787\n",
      "Epoch [26/100], Step [200/938], D Loss: 1.3989, G Loss: 1.7377\n",
      "Epoch [26/100], Step [300/938], D Loss: 1.3154, G Loss: 0.7974\n",
      "Epoch [26/100], Step [400/938], D Loss: 0.9431, G Loss: 1.0215\n",
      "Epoch [26/100], Step [500/938], D Loss: 0.9848, G Loss: 1.4529\n",
      "Epoch [26/100], Step [600/938], D Loss: 0.9975, G Loss: 1.0061\n",
      "Epoch [26/100], Step [700/938], D Loss: 0.8891, G Loss: 1.3280\n",
      "Epoch [26/100], Step [800/938], D Loss: 1.0741, G Loss: 0.9199\n",
      "Epoch [26/100], Step [900/938], D Loss: 1.0897, G Loss: 1.4506\n",
      "Epoch [27/100], Step [0/938], D Loss: 0.9906, G Loss: 1.6762\n",
      "Epoch [27/100], Step [100/938], D Loss: 1.1574, G Loss: 1.9119\n",
      "Epoch [27/100], Step [200/938], D Loss: 0.9771, G Loss: 1.6673\n",
      "Epoch [27/100], Step [300/938], D Loss: 1.0674, G Loss: 1.3701\n",
      "Epoch [27/100], Step [400/938], D Loss: 1.0331, G Loss: 1.8991\n",
      "Epoch [27/100], Step [500/938], D Loss: 1.3003, G Loss: 0.5971\n",
      "Epoch [27/100], Step [600/938], D Loss: 1.3197, G Loss: 2.2864\n",
      "Epoch [27/100], Step [700/938], D Loss: 1.1712, G Loss: 1.6905\n",
      "Epoch [27/100], Step [800/938], D Loss: 1.1297, G Loss: 1.1379\n",
      "Epoch [27/100], Step [900/938], D Loss: 0.9486, G Loss: 1.4926\n",
      "Epoch [28/100], Step [0/938], D Loss: 1.1007, G Loss: 1.4084\n",
      "Epoch [28/100], Step [100/938], D Loss: 1.1253, G Loss: 1.2715\n",
      "Epoch [28/100], Step [200/938], D Loss: 1.1364, G Loss: 0.9468\n",
      "Epoch [28/100], Step [300/938], D Loss: 1.2048, G Loss: 1.1344\n",
      "Epoch [28/100], Step [400/938], D Loss: 1.0999, G Loss: 1.1216\n",
      "Epoch [28/100], Step [500/938], D Loss: 1.1768, G Loss: 0.8286\n",
      "Epoch [28/100], Step [600/938], D Loss: 1.0241, G Loss: 1.3385\n",
      "Epoch [28/100], Step [700/938], D Loss: 1.0928, G Loss: 1.4062\n",
      "Epoch [28/100], Step [800/938], D Loss: 1.0851, G Loss: 1.2645\n",
      "Epoch [28/100], Step [900/938], D Loss: 1.0588, G Loss: 0.9859\n",
      "Epoch [29/100], Step [0/938], D Loss: 0.9660, G Loss: 1.2651\n",
      "Epoch [29/100], Step [100/938], D Loss: 1.0355, G Loss: 0.9229\n",
      "Epoch [29/100], Step [200/938], D Loss: 1.0635, G Loss: 1.4715\n",
      "Epoch [29/100], Step [300/938], D Loss: 1.0444, G Loss: 1.4948\n",
      "Epoch [29/100], Step [400/938], D Loss: 1.0253, G Loss: 1.7146\n",
      "Epoch [29/100], Step [500/938], D Loss: 1.2538, G Loss: 1.1901\n",
      "Epoch [29/100], Step [600/938], D Loss: 1.1250, G Loss: 0.9069\n",
      "Epoch [29/100], Step [700/938], D Loss: 0.9976, G Loss: 1.0997\n",
      "Epoch [29/100], Step [800/938], D Loss: 1.0023, G Loss: 1.0710\n",
      "Epoch [29/100], Step [900/938], D Loss: 1.1802, G Loss: 1.2997\n",
      "Epoch [30/100], Step [0/938], D Loss: 1.1407, G Loss: 1.6053\n",
      "Epoch [30/100], Step [100/938], D Loss: 0.9858, G Loss: 1.2431\n",
      "Epoch [30/100], Step [200/938], D Loss: 1.0204, G Loss: 1.2240\n",
      "Epoch [30/100], Step [300/938], D Loss: 1.0933, G Loss: 0.8874\n",
      "Epoch [30/100], Step [400/938], D Loss: 1.0963, G Loss: 1.3414\n",
      "Epoch [30/100], Step [500/938], D Loss: 1.0174, G Loss: 0.8074\n",
      "Epoch [30/100], Step [600/938], D Loss: 1.2739, G Loss: 1.8627\n",
      "Epoch [30/100], Step [700/938], D Loss: 0.9340, G Loss: 1.5982\n",
      "Epoch [30/100], Step [800/938], D Loss: 1.0914, G Loss: 1.1968\n",
      "Epoch [30/100], Step [900/938], D Loss: 0.9009, G Loss: 1.1627\n",
      "Epoch [31/100], Step [0/938], D Loss: 1.1217, G Loss: 0.7017\n",
      "Epoch [31/100], Step [100/938], D Loss: 1.0727, G Loss: 1.2531\n",
      "Epoch [31/100], Step [200/938], D Loss: 1.1194, G Loss: 1.3145\n",
      "Epoch [31/100], Step [300/938], D Loss: 1.1204, G Loss: 1.2902\n",
      "Epoch [31/100], Step [400/938], D Loss: 1.2186, G Loss: 1.1554\n",
      "Epoch [31/100], Step [500/938], D Loss: 1.0579, G Loss: 1.4060\n",
      "Epoch [31/100], Step [600/938], D Loss: 1.0558, G Loss: 1.8786\n",
      "Epoch [31/100], Step [700/938], D Loss: 1.0462, G Loss: 1.8350\n",
      "Epoch [31/100], Step [800/938], D Loss: 1.1941, G Loss: 0.9330\n",
      "Epoch [31/100], Step [900/938], D Loss: 1.0553, G Loss: 1.1629\n",
      "Epoch [32/100], Step [0/938], D Loss: 1.0961, G Loss: 1.4302\n",
      "Epoch [32/100], Step [100/938], D Loss: 0.9841, G Loss: 1.0234\n",
      "Epoch [32/100], Step [200/938], D Loss: 1.2687, G Loss: 0.5627\n",
      "Epoch [32/100], Step [300/938], D Loss: 0.9723, G Loss: 1.4974\n",
      "Epoch [32/100], Step [400/938], D Loss: 1.0828, G Loss: 0.7389\n",
      "Epoch [32/100], Step [500/938], D Loss: 1.0650, G Loss: 1.3510\n",
      "Epoch [32/100], Step [600/938], D Loss: 1.1664, G Loss: 1.5580\n",
      "Epoch [32/100], Step [700/938], D Loss: 1.1146, G Loss: 1.1608\n",
      "Epoch [32/100], Step [800/938], D Loss: 0.9411, G Loss: 1.0251\n",
      "Epoch [32/100], Step [900/938], D Loss: 1.1207, G Loss: 0.8777\n",
      "Epoch [33/100], Step [0/938], D Loss: 0.9680, G Loss: 1.1359\n",
      "Epoch [33/100], Step [100/938], D Loss: 1.4915, G Loss: 0.6828\n",
      "Epoch [33/100], Step [200/938], D Loss: 1.0557, G Loss: 0.9674\n",
      "Epoch [33/100], Step [300/938], D Loss: 1.1395, G Loss: 1.2839\n",
      "Epoch [33/100], Step [400/938], D Loss: 1.1027, G Loss: 0.6708\n",
      "Epoch [33/100], Step [500/938], D Loss: 0.9730, G Loss: 1.1507\n",
      "Epoch [33/100], Step [600/938], D Loss: 1.0993, G Loss: 0.9001\n",
      "Epoch [33/100], Step [700/938], D Loss: 1.0692, G Loss: 1.8606\n",
      "Epoch [33/100], Step [800/938], D Loss: 1.0525, G Loss: 0.9212\n",
      "Epoch [33/100], Step [900/938], D Loss: 1.0189, G Loss: 1.2565\n",
      "Epoch [34/100], Step [0/938], D Loss: 1.2321, G Loss: 1.2250\n",
      "Epoch [34/100], Step [100/938], D Loss: 1.1497, G Loss: 1.0661\n",
      "Epoch [34/100], Step [200/938], D Loss: 0.9883, G Loss: 1.3462\n",
      "Epoch [34/100], Step [300/938], D Loss: 1.2193, G Loss: 1.0910\n",
      "Epoch [34/100], Step [400/938], D Loss: 1.1782, G Loss: 0.7842\n",
      "Epoch [34/100], Step [500/938], D Loss: 1.1866, G Loss: 0.7175\n",
      "Epoch [34/100], Step [600/938], D Loss: 1.0018, G Loss: 1.0094\n",
      "Epoch [34/100], Step [700/938], D Loss: 1.0095, G Loss: 1.3552\n",
      "Epoch [34/100], Step [800/938], D Loss: 1.0326, G Loss: 1.0506\n",
      "Epoch [34/100], Step [900/938], D Loss: 1.1632, G Loss: 1.6130\n",
      "Epoch [35/100], Step [0/938], D Loss: 1.0371, G Loss: 1.4265\n",
      "Epoch [35/100], Step [100/938], D Loss: 1.0948, G Loss: 0.8959\n",
      "Epoch [35/100], Step [200/938], D Loss: 1.0691, G Loss: 0.8576\n",
      "Epoch [35/100], Step [300/938], D Loss: 0.9715, G Loss: 0.9419\n",
      "Epoch [35/100], Step [400/938], D Loss: 0.9758, G Loss: 1.3544\n",
      "Epoch [35/100], Step [500/938], D Loss: 1.0600, G Loss: 1.4440\n",
      "Epoch [35/100], Step [600/938], D Loss: 1.0631, G Loss: 1.6186\n",
      "Epoch [35/100], Step [700/938], D Loss: 1.0842, G Loss: 1.2531\n",
      "Epoch [35/100], Step [800/938], D Loss: 1.0509, G Loss: 1.4027\n",
      "Epoch [35/100], Step [900/938], D Loss: 1.0088, G Loss: 1.4532\n",
      "Epoch [36/100], Step [0/938], D Loss: 1.3059, G Loss: 1.0905\n",
      "Epoch [36/100], Step [100/938], D Loss: 1.0691, G Loss: 1.5530\n",
      "Epoch [36/100], Step [200/938], D Loss: 1.1916, G Loss: 1.5432\n",
      "Epoch [36/100], Step [300/938], D Loss: 1.1010, G Loss: 1.3851\n",
      "Epoch [36/100], Step [400/938], D Loss: 1.0389, G Loss: 1.5927\n",
      "Epoch [36/100], Step [500/938], D Loss: 0.9825, G Loss: 1.1000\n",
      "Epoch [36/100], Step [600/938], D Loss: 1.2425, G Loss: 0.9324\n",
      "Epoch [36/100], Step [700/938], D Loss: 0.9650, G Loss: 1.5540\n",
      "Epoch [36/100], Step [800/938], D Loss: 1.0745, G Loss: 1.0361\n",
      "Epoch [36/100], Step [900/938], D Loss: 1.2232, G Loss: 0.8214\n",
      "Epoch [37/100], Step [0/938], D Loss: 1.1964, G Loss: 1.1119\n",
      "Epoch [37/100], Step [100/938], D Loss: 1.2091, G Loss: 2.0698\n",
      "Epoch [37/100], Step [200/938], D Loss: 1.0102, G Loss: 1.0048\n",
      "Epoch [37/100], Step [300/938], D Loss: 1.0631, G Loss: 1.2255\n",
      "Epoch [37/100], Step [400/938], D Loss: 1.2517, G Loss: 0.8883\n",
      "Epoch [37/100], Step [500/938], D Loss: 1.1215, G Loss: 0.7725\n",
      "Epoch [37/100], Step [600/938], D Loss: 1.2787, G Loss: 1.3372\n",
      "Epoch [37/100], Step [700/938], D Loss: 1.0769, G Loss: 1.1736\n",
      "Epoch [37/100], Step [800/938], D Loss: 1.0153, G Loss: 1.0420\n",
      "Epoch [37/100], Step [900/938], D Loss: 1.1507, G Loss: 1.2311\n",
      "Epoch [38/100], Step [0/938], D Loss: 1.0313, G Loss: 1.1604\n",
      "Epoch [38/100], Step [100/938], D Loss: 1.1332, G Loss: 1.0535\n",
      "Epoch [38/100], Step [200/938], D Loss: 1.0458, G Loss: 1.7108\n",
      "Epoch [38/100], Step [300/938], D Loss: 0.9352, G Loss: 1.1283\n",
      "Epoch [38/100], Step [400/938], D Loss: 1.2149, G Loss: 1.1836\n",
      "Epoch [38/100], Step [500/938], D Loss: 1.0449, G Loss: 0.9686\n",
      "Epoch [38/100], Step [600/938], D Loss: 0.9893, G Loss: 1.1116\n",
      "Epoch [38/100], Step [700/938], D Loss: 1.0095, G Loss: 1.2893\n",
      "Epoch [38/100], Step [800/938], D Loss: 1.2031, G Loss: 1.2655\n",
      "Epoch [38/100], Step [900/938], D Loss: 1.2858, G Loss: 2.1095\n",
      "Epoch [39/100], Step [0/938], D Loss: 1.0747, G Loss: 1.5617\n",
      "Epoch [39/100], Step [100/938], D Loss: 1.1714, G Loss: 1.2509\n",
      "Epoch [39/100], Step [200/938], D Loss: 1.1295, G Loss: 1.2980\n",
      "Epoch [39/100], Step [300/938], D Loss: 1.2131, G Loss: 0.8677\n",
      "Epoch [39/100], Step [400/938], D Loss: 1.0560, G Loss: 1.3075\n",
      "Epoch [39/100], Step [500/938], D Loss: 1.0189, G Loss: 1.0650\n",
      "Epoch [39/100], Step [600/938], D Loss: 1.1665, G Loss: 1.0240\n",
      "Epoch [39/100], Step [700/938], D Loss: 0.9554, G Loss: 1.3969\n",
      "Epoch [39/100], Step [800/938], D Loss: 1.1832, G Loss: 1.0055\n",
      "Epoch [39/100], Step [900/938], D Loss: 1.1603, G Loss: 0.7156\n",
      "Epoch [40/100], Step [0/938], D Loss: 1.2465, G Loss: 1.5769\n",
      "Epoch [40/100], Step [100/938], D Loss: 1.2284, G Loss: 1.5670\n",
      "Epoch [40/100], Step [200/938], D Loss: 1.1678, G Loss: 1.4304\n",
      "Epoch [40/100], Step [300/938], D Loss: 1.0823, G Loss: 1.1084\n",
      "Epoch [40/100], Step [400/938], D Loss: 1.1569, G Loss: 1.3847\n",
      "Epoch [40/100], Step [500/938], D Loss: 1.1931, G Loss: 0.5619\n",
      "Epoch [40/100], Step [600/938], D Loss: 1.1168, G Loss: 0.7308\n",
      "Epoch [40/100], Step [700/938], D Loss: 1.0585, G Loss: 1.1025\n",
      "Epoch [40/100], Step [800/938], D Loss: 1.1342, G Loss: 0.8019\n",
      "Epoch [40/100], Step [900/938], D Loss: 0.8658, G Loss: 1.5407\n",
      "Epoch [41/100], Step [0/938], D Loss: 1.0567, G Loss: 1.5563\n",
      "Epoch [41/100], Step [100/938], D Loss: 0.9192, G Loss: 0.9726\n",
      "Epoch [41/100], Step [200/938], D Loss: 1.1236, G Loss: 1.1606\n",
      "Epoch [41/100], Step [300/938], D Loss: 1.0479, G Loss: 1.6722\n",
      "Epoch [41/100], Step [400/938], D Loss: 1.2061, G Loss: 1.5035\n",
      "Epoch [41/100], Step [500/938], D Loss: 1.0301, G Loss: 1.2663\n",
      "Epoch [41/100], Step [600/938], D Loss: 1.1498, G Loss: 1.0758\n",
      "Epoch [41/100], Step [700/938], D Loss: 1.1219, G Loss: 1.0365\n",
      "Epoch [41/100], Step [800/938], D Loss: 1.2485, G Loss: 1.2215\n",
      "Epoch [41/100], Step [900/938], D Loss: 1.1730, G Loss: 0.8810\n",
      "Epoch [42/100], Step [0/938], D Loss: 1.0279, G Loss: 1.4179\n",
      "Epoch [42/100], Step [100/938], D Loss: 1.2620, G Loss: 0.7874\n",
      "Epoch [42/100], Step [200/938], D Loss: 0.9808, G Loss: 1.2779\n",
      "Epoch [42/100], Step [300/938], D Loss: 1.1781, G Loss: 1.5523\n",
      "Epoch [42/100], Step [400/938], D Loss: 1.0689, G Loss: 1.3557\n",
      "Epoch [42/100], Step [500/938], D Loss: 1.1204, G Loss: 1.1930\n",
      "Epoch [42/100], Step [600/938], D Loss: 1.0397, G Loss: 0.8831\n",
      "Epoch [42/100], Step [700/938], D Loss: 1.2610, G Loss: 1.0166\n",
      "Epoch [42/100], Step [800/938], D Loss: 1.0016, G Loss: 1.3444\n",
      "Epoch [42/100], Step [900/938], D Loss: 1.2195, G Loss: 1.3120\n",
      "Epoch [43/100], Step [0/938], D Loss: 1.1837, G Loss: 0.7308\n",
      "Epoch [43/100], Step [100/938], D Loss: 1.2366, G Loss: 1.6811\n",
      "Epoch [43/100], Step [200/938], D Loss: 1.0473, G Loss: 1.4609\n",
      "Epoch [43/100], Step [300/938], D Loss: 1.1559, G Loss: 1.0019\n",
      "Epoch [43/100], Step [400/938], D Loss: 0.9534, G Loss: 1.0823\n",
      "Epoch [43/100], Step [500/938], D Loss: 1.2662, G Loss: 0.6471\n",
      "Epoch [43/100], Step [600/938], D Loss: 1.0242, G Loss: 1.2261\n",
      "Epoch [43/100], Step [700/938], D Loss: 1.1170, G Loss: 1.5775\n",
      "Epoch [43/100], Step [800/938], D Loss: 1.1697, G Loss: 0.7383\n",
      "Epoch [43/100], Step [900/938], D Loss: 1.1727, G Loss: 1.1086\n",
      "Epoch [44/100], Step [0/938], D Loss: 1.0408, G Loss: 1.0745\n",
      "Epoch [44/100], Step [100/938], D Loss: 1.1402, G Loss: 1.0617\n",
      "Epoch [44/100], Step [200/938], D Loss: 1.2893, G Loss: 1.1004\n",
      "Epoch [44/100], Step [300/938], D Loss: 1.1403, G Loss: 0.8799\n",
      "Epoch [44/100], Step [400/938], D Loss: 1.2051, G Loss: 1.5440\n",
      "Epoch [44/100], Step [500/938], D Loss: 1.3640, G Loss: 1.4573\n",
      "Epoch [44/100], Step [600/938], D Loss: 1.0451, G Loss: 1.1071\n",
      "Epoch [44/100], Step [700/938], D Loss: 1.0975, G Loss: 1.4246\n",
      "Epoch [44/100], Step [800/938], D Loss: 1.0458, G Loss: 0.9883\n",
      "Epoch [44/100], Step [900/938], D Loss: 1.1206, G Loss: 0.7521\n",
      "Epoch [45/100], Step [0/938], D Loss: 1.1920, G Loss: 1.0863\n",
      "Epoch [45/100], Step [100/938], D Loss: 1.1579, G Loss: 1.6161\n",
      "Epoch [45/100], Step [200/938], D Loss: 1.2044, G Loss: 1.1435\n",
      "Epoch [45/100], Step [300/938], D Loss: 1.1552, G Loss: 1.2753\n",
      "Epoch [45/100], Step [400/938], D Loss: 1.0211, G Loss: 1.3754\n",
      "Epoch [45/100], Step [500/938], D Loss: 1.2480, G Loss: 0.8405\n",
      "Epoch [45/100], Step [600/938], D Loss: 1.0053, G Loss: 0.8187\n",
      "Epoch [45/100], Step [700/938], D Loss: 1.0875, G Loss: 1.4535\n",
      "Epoch [45/100], Step [800/938], D Loss: 1.1440, G Loss: 1.2010\n",
      "Epoch [45/100], Step [900/938], D Loss: 1.1586, G Loss: 0.9177\n",
      "Epoch [46/100], Step [0/938], D Loss: 1.0427, G Loss: 1.6229\n",
      "Epoch [46/100], Step [100/938], D Loss: 1.1128, G Loss: 0.9434\n",
      "Epoch [46/100], Step [200/938], D Loss: 1.1495, G Loss: 1.1000\n",
      "Epoch [46/100], Step [300/938], D Loss: 1.0905, G Loss: 0.8003\n",
      "Epoch [46/100], Step [400/938], D Loss: 1.1578, G Loss: 1.0829\n",
      "Epoch [46/100], Step [500/938], D Loss: 1.1699, G Loss: 1.0971\n",
      "Epoch [46/100], Step [600/938], D Loss: 1.0549, G Loss: 0.9452\n",
      "Epoch [46/100], Step [700/938], D Loss: 1.1027, G Loss: 1.3262\n",
      "Epoch [46/100], Step [800/938], D Loss: 1.1597, G Loss: 0.5060\n",
      "Epoch [46/100], Step [900/938], D Loss: 0.9665, G Loss: 0.9839\n",
      "Epoch [47/100], Step [0/938], D Loss: 0.9418, G Loss: 1.3640\n",
      "Epoch [47/100], Step [100/938], D Loss: 1.1872, G Loss: 1.2911\n",
      "Epoch [47/100], Step [200/938], D Loss: 0.9219, G Loss: 1.2004\n",
      "Epoch [47/100], Step [300/938], D Loss: 1.1681, G Loss: 0.7038\n",
      "Epoch [47/100], Step [400/938], D Loss: 1.1149, G Loss: 1.9942\n",
      "Epoch [47/100], Step [500/938], D Loss: 1.0821, G Loss: 1.4349\n",
      "Epoch [47/100], Step [600/938], D Loss: 1.1707, G Loss: 1.1997\n",
      "Epoch [47/100], Step [700/938], D Loss: 1.3259, G Loss: 1.2753\n",
      "Epoch [47/100], Step [800/938], D Loss: 1.2203, G Loss: 0.8320\n",
      "Epoch [47/100], Step [900/938], D Loss: 1.1651, G Loss: 1.2259\n",
      "Epoch [48/100], Step [0/938], D Loss: 1.1580, G Loss: 1.2270\n",
      "Epoch [48/100], Step [100/938], D Loss: 1.2131, G Loss: 0.7372\n",
      "Epoch [48/100], Step [200/938], D Loss: 1.1447, G Loss: 1.3048\n",
      "Epoch [48/100], Step [300/938], D Loss: 1.0107, G Loss: 1.1669\n",
      "Epoch [48/100], Step [400/938], D Loss: 1.2568, G Loss: 1.2901\n",
      "Epoch [48/100], Step [500/938], D Loss: 1.0929, G Loss: 0.8697\n",
      "Epoch [48/100], Step [600/938], D Loss: 1.3763, G Loss: 1.8818\n",
      "Epoch [48/100], Step [700/938], D Loss: 1.1178, G Loss: 1.1825\n",
      "Epoch [48/100], Step [800/938], D Loss: 1.2330, G Loss: 0.7918\n",
      "Epoch [48/100], Step [900/938], D Loss: 0.9740, G Loss: 1.1022\n",
      "Epoch [49/100], Step [0/938], D Loss: 1.1281, G Loss: 0.8176\n",
      "Epoch [49/100], Step [100/938], D Loss: 1.0405, G Loss: 1.3532\n",
      "Epoch [49/100], Step [200/938], D Loss: 1.0725, G Loss: 1.4007\n",
      "Epoch [49/100], Step [300/938], D Loss: 1.2481, G Loss: 0.7457\n",
      "Epoch [49/100], Step [400/938], D Loss: 1.2156, G Loss: 1.5688\n",
      "Epoch [49/100], Step [500/938], D Loss: 1.2748, G Loss: 1.2593\n",
      "Epoch [49/100], Step [600/938], D Loss: 1.0953, G Loss: 1.2613\n",
      "Epoch [49/100], Step [700/938], D Loss: 1.0533, G Loss: 1.1874\n",
      "Epoch [49/100], Step [800/938], D Loss: 1.1852, G Loss: 0.7431\n",
      "Epoch [49/100], Step [900/938], D Loss: 1.2313, G Loss: 1.2312\n",
      "Epoch [50/100], Step [0/938], D Loss: 1.1037, G Loss: 0.6462\n",
      "Epoch [50/100], Step [100/938], D Loss: 1.0336, G Loss: 0.7914\n",
      "Epoch [50/100], Step [200/938], D Loss: 1.1639, G Loss: 1.4383\n",
      "Epoch [50/100], Step [300/938], D Loss: 1.2302, G Loss: 0.8501\n",
      "Epoch [50/100], Step [400/938], D Loss: 1.2836, G Loss: 1.7167\n",
      "Epoch [50/100], Step [500/938], D Loss: 1.0693, G Loss: 1.4624\n",
      "Epoch [50/100], Step [600/938], D Loss: 0.9980, G Loss: 1.5364\n",
      "Epoch [50/100], Step [700/938], D Loss: 1.1581, G Loss: 1.5394\n",
      "Epoch [50/100], Step [800/938], D Loss: 1.0756, G Loss: 1.4807\n",
      "Epoch [50/100], Step [900/938], D Loss: 1.2123, G Loss: 1.8127\n",
      "Epoch [51/100], Step [0/938], D Loss: 1.0709, G Loss: 0.9581\n",
      "Epoch [51/100], Step [100/938], D Loss: 1.0641, G Loss: 1.3761\n",
      "Epoch [51/100], Step [200/938], D Loss: 1.0855, G Loss: 0.9597\n",
      "Epoch [51/100], Step [300/938], D Loss: 1.0025, G Loss: 1.2448\n",
      "Epoch [51/100], Step [400/938], D Loss: 1.1167, G Loss: 0.6368\n",
      "Epoch [51/100], Step [500/938], D Loss: 0.9584, G Loss: 1.3088\n",
      "Epoch [51/100], Step [600/938], D Loss: 1.1345, G Loss: 1.4267\n",
      "Epoch [51/100], Step [700/938], D Loss: 1.1110, G Loss: 1.2004\n",
      "Epoch [51/100], Step [800/938], D Loss: 1.1859, G Loss: 1.2912\n",
      "Epoch [51/100], Step [900/938], D Loss: 1.1409, G Loss: 1.3796\n",
      "Epoch [52/100], Step [0/938], D Loss: 1.1519, G Loss: 0.9218\n",
      "Epoch [52/100], Step [100/938], D Loss: 1.1572, G Loss: 1.0111\n",
      "Epoch [52/100], Step [200/938], D Loss: 1.1768, G Loss: 1.4078\n",
      "Epoch [52/100], Step [300/938], D Loss: 1.1291, G Loss: 1.2906\n",
      "Epoch [52/100], Step [400/938], D Loss: 1.2070, G Loss: 0.7899\n",
      "Epoch [52/100], Step [500/938], D Loss: 1.1259, G Loss: 0.9012\n",
      "Epoch [52/100], Step [600/938], D Loss: 1.0619, G Loss: 1.1276\n",
      "Epoch [52/100], Step [700/938], D Loss: 1.0825, G Loss: 1.3959\n",
      "Epoch [52/100], Step [800/938], D Loss: 1.1053, G Loss: 1.1761\n",
      "Epoch [52/100], Step [900/938], D Loss: 1.3297, G Loss: 1.0730\n",
      "Epoch [53/100], Step [0/938], D Loss: 1.2571, G Loss: 1.0716\n",
      "Epoch [53/100], Step [100/938], D Loss: 0.9930, G Loss: 1.4904\n",
      "Epoch [53/100], Step [200/938], D Loss: 1.1736, G Loss: 0.9277\n",
      "Epoch [53/100], Step [300/938], D Loss: 1.0333, G Loss: 1.0610\n",
      "Epoch [53/100], Step [400/938], D Loss: 1.3240, G Loss: 1.4318\n",
      "Epoch [53/100], Step [500/938], D Loss: 1.2451, G Loss: 1.1961\n",
      "Epoch [53/100], Step [600/938], D Loss: 1.1367, G Loss: 0.9799\n",
      "Epoch [53/100], Step [700/938], D Loss: 1.2453, G Loss: 0.9649\n",
      "Epoch [53/100], Step [800/938], D Loss: 1.1198, G Loss: 0.9524\n",
      "Epoch [53/100], Step [900/938], D Loss: 1.1708, G Loss: 0.6964\n",
      "Epoch [54/100], Step [0/938], D Loss: 1.0564, G Loss: 1.3174\n",
      "Epoch [54/100], Step [100/938], D Loss: 1.1923, G Loss: 0.5832\n",
      "Epoch [54/100], Step [200/938], D Loss: 0.9481, G Loss: 1.2644\n",
      "Epoch [54/100], Step [300/938], D Loss: 1.2313, G Loss: 0.8711\n",
      "Epoch [54/100], Step [400/938], D Loss: 1.2670, G Loss: 1.3102\n",
      "Epoch [54/100], Step [500/938], D Loss: 1.1666, G Loss: 0.7365\n",
      "Epoch [54/100], Step [600/938], D Loss: 1.0801, G Loss: 1.1748\n",
      "Epoch [54/100], Step [700/938], D Loss: 1.2346, G Loss: 1.5169\n",
      "Epoch [54/100], Step [800/938], D Loss: 1.2059, G Loss: 1.4995\n",
      "Epoch [54/100], Step [900/938], D Loss: 1.2192, G Loss: 0.6911\n",
      "Epoch [55/100], Step [0/938], D Loss: 1.0102, G Loss: 1.4584\n",
      "Epoch [55/100], Step [100/938], D Loss: 1.0914, G Loss: 1.3407\n",
      "Epoch [55/100], Step [200/938], D Loss: 1.1280, G Loss: 0.6170\n",
      "Epoch [55/100], Step [300/938], D Loss: 0.9759, G Loss: 1.5762\n",
      "Epoch [55/100], Step [400/938], D Loss: 1.1621, G Loss: 1.7494\n",
      "Epoch [55/100], Step [500/938], D Loss: 1.0567, G Loss: 1.1851\n",
      "Epoch [55/100], Step [600/938], D Loss: 1.1511, G Loss: 1.6763\n",
      "Epoch [55/100], Step [700/938], D Loss: 1.1666, G Loss: 1.0950\n",
      "Epoch [55/100], Step [800/938], D Loss: 1.1527, G Loss: 1.3589\n",
      "Epoch [55/100], Step [900/938], D Loss: 1.2342, G Loss: 0.6615\n",
      "Epoch [56/100], Step [0/938], D Loss: 1.1705, G Loss: 1.1899\n",
      "Epoch [56/100], Step [100/938], D Loss: 1.0980, G Loss: 1.8787\n",
      "Epoch [56/100], Step [200/938], D Loss: 1.1788, G Loss: 0.9608\n",
      "Epoch [56/100], Step [300/938], D Loss: 1.1538, G Loss: 1.1165\n",
      "Epoch [56/100], Step [400/938], D Loss: 1.1256, G Loss: 1.0887\n",
      "Epoch [56/100], Step [500/938], D Loss: 1.0685, G Loss: 1.4285\n",
      "Epoch [56/100], Step [600/938], D Loss: 0.9134, G Loss: 1.0655\n",
      "Epoch [56/100], Step [700/938], D Loss: 1.2114, G Loss: 0.9571\n",
      "Epoch [56/100], Step [800/938], D Loss: 1.0807, G Loss: 1.4062\n",
      "Epoch [56/100], Step [900/938], D Loss: 1.1892, G Loss: 1.2941\n",
      "Epoch [57/100], Step [0/938], D Loss: 1.0177, G Loss: 1.3727\n",
      "Epoch [57/100], Step [100/938], D Loss: 1.1760, G Loss: 0.6728\n",
      "Epoch [57/100], Step [200/938], D Loss: 0.9867, G Loss: 1.2130\n",
      "Epoch [57/100], Step [300/938], D Loss: 1.0485, G Loss: 1.4843\n",
      "Epoch [57/100], Step [400/938], D Loss: 0.9360, G Loss: 0.9465\n",
      "Epoch [57/100], Step [500/938], D Loss: 1.2449, G Loss: 1.2615\n",
      "Epoch [57/100], Step [600/938], D Loss: 1.1010, G Loss: 0.9353\n",
      "Epoch [57/100], Step [700/938], D Loss: 1.1087, G Loss: 1.2653\n",
      "Epoch [57/100], Step [800/938], D Loss: 1.1548, G Loss: 0.9746\n",
      "Epoch [57/100], Step [900/938], D Loss: 1.3600, G Loss: 0.8440\n",
      "Epoch [58/100], Step [0/938], D Loss: 1.0803, G Loss: 0.7754\n",
      "Epoch [58/100], Step [100/938], D Loss: 1.1226, G Loss: 1.2304\n",
      "Epoch [58/100], Step [200/938], D Loss: 1.0838, G Loss: 1.3516\n",
      "Epoch [58/100], Step [300/938], D Loss: 1.2071, G Loss: 1.5046\n",
      "Epoch [58/100], Step [400/938], D Loss: 1.0005, G Loss: 0.9096\n",
      "Epoch [58/100], Step [500/938], D Loss: 1.2371, G Loss: 0.7853\n",
      "Epoch [58/100], Step [600/938], D Loss: 1.1638, G Loss: 1.0130\n",
      "Epoch [58/100], Step [700/938], D Loss: 1.1385, G Loss: 0.9005\n",
      "Epoch [58/100], Step [800/938], D Loss: 1.2456, G Loss: 1.0429\n",
      "Epoch [58/100], Step [900/938], D Loss: 1.0539, G Loss: 1.0956\n",
      "Epoch [59/100], Step [0/938], D Loss: 1.0060, G Loss: 1.3132\n",
      "Epoch [59/100], Step [100/938], D Loss: 1.1416, G Loss: 1.3498\n",
      "Epoch [59/100], Step [200/938], D Loss: 1.0764, G Loss: 0.9576\n",
      "Epoch [59/100], Step [300/938], D Loss: 1.0914, G Loss: 0.8642\n",
      "Epoch [59/100], Step [400/938], D Loss: 1.0577, G Loss: 0.8659\n",
      "Epoch [59/100], Step [500/938], D Loss: 1.3142, G Loss: 0.7131\n",
      "Epoch [59/100], Step [600/938], D Loss: 1.3938, G Loss: 1.7798\n",
      "Epoch [59/100], Step [700/938], D Loss: 1.0352, G Loss: 0.8725\n",
      "Epoch [59/100], Step [800/938], D Loss: 1.0467, G Loss: 1.1126\n",
      "Epoch [59/100], Step [900/938], D Loss: 0.9985, G Loss: 1.1319\n",
      "Epoch [60/100], Step [0/938], D Loss: 1.2811, G Loss: 0.6002\n",
      "Epoch [60/100], Step [100/938], D Loss: 1.2590, G Loss: 1.1450\n",
      "Epoch [60/100], Step [200/938], D Loss: 1.0564, G Loss: 0.9485\n",
      "Epoch [60/100], Step [300/938], D Loss: 0.9798, G Loss: 0.8613\n",
      "Epoch [60/100], Step [400/938], D Loss: 1.0546, G Loss: 1.0176\n",
      "Epoch [60/100], Step [500/938], D Loss: 1.2359, G Loss: 0.7868\n",
      "Epoch [60/100], Step [600/938], D Loss: 1.1585, G Loss: 0.9290\n",
      "Epoch [60/100], Step [700/938], D Loss: 1.0094, G Loss: 1.3695\n",
      "Epoch [60/100], Step [800/938], D Loss: 1.1350, G Loss: 1.0245\n",
      "Epoch [60/100], Step [900/938], D Loss: 1.2747, G Loss: 1.5851\n",
      "Epoch [61/100], Step [0/938], D Loss: 1.2358, G Loss: 0.7328\n",
      "Epoch [61/100], Step [100/938], D Loss: 1.2394, G Loss: 1.0262\n",
      "Epoch [61/100], Step [200/938], D Loss: 1.3154, G Loss: 1.4521\n",
      "Epoch [61/100], Step [300/938], D Loss: 1.0521, G Loss: 1.2960\n",
      "Epoch [61/100], Step [400/938], D Loss: 1.1212, G Loss: 1.2821\n",
      "Epoch [61/100], Step [500/938], D Loss: 1.0774, G Loss: 0.8931\n",
      "Epoch [61/100], Step [600/938], D Loss: 1.1738, G Loss: 1.5363\n",
      "Epoch [61/100], Step [700/938], D Loss: 1.2484, G Loss: 0.7707\n",
      "Epoch [61/100], Step [800/938], D Loss: 1.1628, G Loss: 1.1060\n",
      "Epoch [61/100], Step [900/938], D Loss: 0.9972, G Loss: 0.9221\n",
      "Epoch [62/100], Step [0/938], D Loss: 1.0415, G Loss: 1.0237\n",
      "Epoch [62/100], Step [100/938], D Loss: 1.2075, G Loss: 0.9404\n",
      "Epoch [62/100], Step [200/938], D Loss: 1.0703, G Loss: 0.7714\n",
      "Epoch [62/100], Step [300/938], D Loss: 1.2554, G Loss: 1.7079\n",
      "Epoch [62/100], Step [400/938], D Loss: 1.1212, G Loss: 1.0764\n",
      "Epoch [62/100], Step [500/938], D Loss: 1.0900, G Loss: 0.7719\n",
      "Epoch [62/100], Step [600/938], D Loss: 0.9845, G Loss: 1.5095\n",
      "Epoch [62/100], Step [700/938], D Loss: 1.1577, G Loss: 1.0186\n",
      "Epoch [62/100], Step [800/938], D Loss: 1.2878, G Loss: 1.4957\n",
      "Epoch [62/100], Step [900/938], D Loss: 1.1320, G Loss: 1.1330\n",
      "Epoch [63/100], Step [0/938], D Loss: 1.0901, G Loss: 1.2182\n",
      "Epoch [63/100], Step [100/938], D Loss: 1.1573, G Loss: 1.0323\n",
      "Epoch [63/100], Step [200/938], D Loss: 1.0879, G Loss: 0.7912\n",
      "Epoch [63/100], Step [300/938], D Loss: 1.0562, G Loss: 0.7832\n",
      "Epoch [63/100], Step [400/938], D Loss: 1.1035, G Loss: 1.6430\n",
      "Epoch [63/100], Step [500/938], D Loss: 1.1941, G Loss: 1.0050\n",
      "Epoch [63/100], Step [600/938], D Loss: 1.0288, G Loss: 1.1252\n",
      "Epoch [63/100], Step [700/938], D Loss: 1.2234, G Loss: 0.9693\n",
      "Epoch [63/100], Step [800/938], D Loss: 1.1528, G Loss: 1.1337\n",
      "Epoch [63/100], Step [900/938], D Loss: 1.2926, G Loss: 0.5663\n",
      "Epoch [64/100], Step [0/938], D Loss: 1.0184, G Loss: 1.2824\n",
      "Epoch [64/100], Step [100/938], D Loss: 1.2144, G Loss: 1.6559\n",
      "Epoch [64/100], Step [200/938], D Loss: 1.3038, G Loss: 0.6358\n",
      "Epoch [64/100], Step [300/938], D Loss: 1.1898, G Loss: 1.4049\n",
      "Epoch [64/100], Step [400/938], D Loss: 1.1517, G Loss: 1.1085\n",
      "Epoch [64/100], Step [500/938], D Loss: 1.2420, G Loss: 1.4261\n",
      "Epoch [64/100], Step [600/938], D Loss: 1.2209, G Loss: 1.7536\n",
      "Epoch [64/100], Step [700/938], D Loss: 1.0080, G Loss: 1.4972\n",
      "Epoch [64/100], Step [800/938], D Loss: 1.0962, G Loss: 1.1383\n",
      "Epoch [64/100], Step [900/938], D Loss: 1.2313, G Loss: 0.9331\n",
      "Epoch [65/100], Step [0/938], D Loss: 1.3911, G Loss: 0.5394\n",
      "Epoch [65/100], Step [100/938], D Loss: 1.1066, G Loss: 0.8244\n",
      "Epoch [65/100], Step [200/938], D Loss: 1.2530, G Loss: 0.9649\n",
      "Epoch [65/100], Step [300/938], D Loss: 1.2501, G Loss: 1.1952\n",
      "Epoch [65/100], Step [400/938], D Loss: 1.1875, G Loss: 1.0289\n",
      "Epoch [65/100], Step [500/938], D Loss: 1.0762, G Loss: 0.9974\n",
      "Epoch [65/100], Step [600/938], D Loss: 1.1052, G Loss: 1.2633\n",
      "Epoch [65/100], Step [700/938], D Loss: 1.1702, G Loss: 0.8760\n",
      "Epoch [65/100], Step [800/938], D Loss: 1.0393, G Loss: 0.9578\n",
      "Epoch [65/100], Step [900/938], D Loss: 0.9906, G Loss: 1.2424\n",
      "Epoch [66/100], Step [0/938], D Loss: 0.9863, G Loss: 1.1932\n",
      "Epoch [66/100], Step [100/938], D Loss: 1.1977, G Loss: 0.8938\n",
      "Epoch [66/100], Step [200/938], D Loss: 1.1179, G Loss: 1.0931\n",
      "Epoch [66/100], Step [300/938], D Loss: 1.0341, G Loss: 1.2575\n",
      "Epoch [66/100], Step [400/938], D Loss: 1.0738, G Loss: 1.6621\n",
      "Epoch [66/100], Step [500/938], D Loss: 1.1197, G Loss: 0.9299\n",
      "Epoch [66/100], Step [600/938], D Loss: 1.1387, G Loss: 1.0651\n",
      "Epoch [66/100], Step [700/938], D Loss: 1.1279, G Loss: 1.1765\n",
      "Epoch [66/100], Step [800/938], D Loss: 1.2520, G Loss: 1.1670\n",
      "Epoch [66/100], Step [900/938], D Loss: 1.1015, G Loss: 0.8875\n",
      "Epoch [67/100], Step [0/938], D Loss: 1.0726, G Loss: 1.3652\n",
      "Epoch [67/100], Step [100/938], D Loss: 1.0445, G Loss: 1.4825\n",
      "Epoch [67/100], Step [200/938], D Loss: 1.1010, G Loss: 0.8904\n",
      "Epoch [67/100], Step [300/938], D Loss: 1.1467, G Loss: 0.9366\n",
      "Epoch [67/100], Step [400/938], D Loss: 1.0805, G Loss: 1.2982\n",
      "Epoch [67/100], Step [500/938], D Loss: 1.2815, G Loss: 0.6949\n",
      "Epoch [67/100], Step [600/938], D Loss: 1.0816, G Loss: 1.0333\n",
      "Epoch [67/100], Step [700/938], D Loss: 1.1877, G Loss: 0.8467\n",
      "Epoch [67/100], Step [800/938], D Loss: 1.0164, G Loss: 1.1432\n",
      "Epoch [67/100], Step [900/938], D Loss: 1.1243, G Loss: 1.3249\n",
      "Epoch [68/100], Step [0/938], D Loss: 1.3642, G Loss: 1.7664\n",
      "Epoch [68/100], Step [100/938], D Loss: 1.1725, G Loss: 1.0886\n",
      "Epoch [68/100], Step [200/938], D Loss: 1.1012, G Loss: 1.0491\n",
      "Epoch [68/100], Step [300/938], D Loss: 1.0841, G Loss: 0.9070\n",
      "Epoch [68/100], Step [400/938], D Loss: 1.1385, G Loss: 1.1340\n",
      "Epoch [68/100], Step [500/938], D Loss: 0.9664, G Loss: 1.3733\n",
      "Epoch [68/100], Step [600/938], D Loss: 1.1394, G Loss: 1.2945\n",
      "Epoch [68/100], Step [700/938], D Loss: 1.2951, G Loss: 0.7495\n",
      "Epoch [68/100], Step [800/938], D Loss: 1.1600, G Loss: 1.5227\n",
      "Epoch [68/100], Step [900/938], D Loss: 1.1913, G Loss: 1.1527\n",
      "Epoch [69/100], Step [0/938], D Loss: 1.1194, G Loss: 0.9498\n",
      "Epoch [69/100], Step [100/938], D Loss: 1.1519, G Loss: 1.4430\n",
      "Epoch [69/100], Step [200/938], D Loss: 1.1842, G Loss: 1.6481\n",
      "Epoch [69/100], Step [300/938], D Loss: 1.2752, G Loss: 0.6493\n",
      "Epoch [69/100], Step [400/938], D Loss: 1.0250, G Loss: 1.1098\n",
      "Epoch [69/100], Step [500/938], D Loss: 1.2110, G Loss: 0.9046\n",
      "Epoch [69/100], Step [600/938], D Loss: 1.1782, G Loss: 1.2557\n",
      "Epoch [69/100], Step [700/938], D Loss: 1.1163, G Loss: 1.4140\n",
      "Epoch [69/100], Step [800/938], D Loss: 1.1640, G Loss: 1.3023\n",
      "Epoch [69/100], Step [900/938], D Loss: 0.9489, G Loss: 1.3367\n",
      "Epoch [70/100], Step [0/938], D Loss: 1.2255, G Loss: 0.6920\n",
      "Epoch [70/100], Step [100/938], D Loss: 1.1512, G Loss: 0.9838\n",
      "Epoch [70/100], Step [200/938], D Loss: 1.0525, G Loss: 0.9238\n",
      "Epoch [70/100], Step [300/938], D Loss: 1.0543, G Loss: 1.2559\n",
      "Epoch [70/100], Step [400/938], D Loss: 1.0440, G Loss: 1.1573\n",
      "Epoch [70/100], Step [500/938], D Loss: 1.1399, G Loss: 0.6478\n",
      "Epoch [70/100], Step [600/938], D Loss: 1.1569, G Loss: 0.9317\n",
      "Epoch [70/100], Step [700/938], D Loss: 1.0787, G Loss: 1.4439\n",
      "Epoch [70/100], Step [800/938], D Loss: 1.1290, G Loss: 1.3942\n",
      "Epoch [70/100], Step [900/938], D Loss: 1.0397, G Loss: 0.8403\n",
      "Epoch [71/100], Step [0/938], D Loss: 1.2685, G Loss: 1.5066\n",
      "Epoch [71/100], Step [100/938], D Loss: 1.1817, G Loss: 0.6356\n",
      "Epoch [71/100], Step [200/938], D Loss: 1.1622, G Loss: 1.3571\n",
      "Epoch [71/100], Step [300/938], D Loss: 1.2593, G Loss: 1.6889\n",
      "Epoch [71/100], Step [400/938], D Loss: 1.1061, G Loss: 1.4898\n",
      "Epoch [71/100], Step [500/938], D Loss: 1.2106, G Loss: 1.7672\n",
      "Epoch [71/100], Step [600/938], D Loss: 1.1091, G Loss: 0.8884\n",
      "Epoch [71/100], Step [700/938], D Loss: 1.0515, G Loss: 1.0830\n",
      "Epoch [71/100], Step [800/938], D Loss: 1.1812, G Loss: 1.4115\n",
      "Epoch [71/100], Step [900/938], D Loss: 1.1147, G Loss: 0.6706\n",
      "Epoch [72/100], Step [0/938], D Loss: 1.1698, G Loss: 1.3832\n",
      "Epoch [72/100], Step [100/938], D Loss: 1.2231, G Loss: 1.7428\n",
      "Epoch [72/100], Step [200/938], D Loss: 1.2680, G Loss: 0.7324\n",
      "Epoch [72/100], Step [300/938], D Loss: 0.9682, G Loss: 1.2064\n",
      "Epoch [72/100], Step [400/938], D Loss: 1.1502, G Loss: 1.2369\n",
      "Epoch [72/100], Step [500/938], D Loss: 1.1267, G Loss: 1.0059\n",
      "Epoch [72/100], Step [600/938], D Loss: 1.1194, G Loss: 1.0749\n",
      "Epoch [72/100], Step [700/938], D Loss: 1.1683, G Loss: 1.6228\n",
      "Epoch [72/100], Step [800/938], D Loss: 1.2584, G Loss: 0.8032\n",
      "Epoch [72/100], Step [900/938], D Loss: 1.2159, G Loss: 0.9293\n",
      "Epoch [73/100], Step [0/938], D Loss: 1.1789, G Loss: 1.8891\n",
      "Epoch [73/100], Step [100/938], D Loss: 1.2545, G Loss: 0.7001\n",
      "Epoch [73/100], Step [200/938], D Loss: 1.0667, G Loss: 1.6622\n",
      "Epoch [73/100], Step [300/938], D Loss: 1.0491, G Loss: 0.8855\n",
      "Epoch [73/100], Step [400/938], D Loss: 0.9528, G Loss: 0.9608\n",
      "Epoch [73/100], Step [500/938], D Loss: 1.1720, G Loss: 1.4290\n",
      "Epoch [73/100], Step [600/938], D Loss: 1.1089, G Loss: 0.8249\n",
      "Epoch [73/100], Step [700/938], D Loss: 1.0645, G Loss: 1.4877\n",
      "Epoch [73/100], Step [800/938], D Loss: 1.0179, G Loss: 1.1683\n",
      "Epoch [73/100], Step [900/938], D Loss: 1.0102, G Loss: 1.1536\n",
      "Epoch [74/100], Step [0/938], D Loss: 1.3700, G Loss: 0.6287\n",
      "Epoch [74/100], Step [100/938], D Loss: 1.1716, G Loss: 1.0011\n",
      "Epoch [74/100], Step [200/938], D Loss: 1.0277, G Loss: 1.4834\n",
      "Epoch [74/100], Step [300/938], D Loss: 1.2786, G Loss: 1.4131\n",
      "Epoch [74/100], Step [400/938], D Loss: 1.1643, G Loss: 1.2147\n",
      "Epoch [74/100], Step [500/938], D Loss: 1.2143, G Loss: 1.1934\n",
      "Epoch [74/100], Step [600/938], D Loss: 1.0292, G Loss: 1.2868\n",
      "Epoch [74/100], Step [700/938], D Loss: 1.0897, G Loss: 0.9504\n",
      "Epoch [74/100], Step [800/938], D Loss: 1.0750, G Loss: 1.1335\n",
      "Epoch [74/100], Step [900/938], D Loss: 1.2140, G Loss: 0.6372\n",
      "Epoch [75/100], Step [0/938], D Loss: 1.1137, G Loss: 1.0524\n",
      "Epoch [75/100], Step [100/938], D Loss: 1.1262, G Loss: 1.2327\n",
      "Epoch [75/100], Step [200/938], D Loss: 1.0581, G Loss: 1.1981\n",
      "Epoch [75/100], Step [300/938], D Loss: 0.9969, G Loss: 1.2689\n",
      "Epoch [75/100], Step [400/938], D Loss: 1.1230, G Loss: 1.0636\n",
      "Epoch [75/100], Step [500/938], D Loss: 1.0528, G Loss: 1.1248\n",
      "Epoch [75/100], Step [600/938], D Loss: 1.0743, G Loss: 1.2135\n",
      "Epoch [75/100], Step [700/938], D Loss: 1.0462, G Loss: 1.3654\n",
      "Epoch [75/100], Step [800/938], D Loss: 1.2711, G Loss: 0.7245\n",
      "Epoch [75/100], Step [900/938], D Loss: 1.1078, G Loss: 1.1727\n",
      "Epoch [76/100], Step [0/938], D Loss: 1.0651, G Loss: 1.2375\n",
      "Epoch [76/100], Step [100/938], D Loss: 0.9907, G Loss: 1.2043\n",
      "Epoch [76/100], Step [200/938], D Loss: 1.1658, G Loss: 1.5100\n",
      "Epoch [76/100], Step [300/938], D Loss: 1.3245, G Loss: 0.5922\n",
      "Epoch [76/100], Step [400/938], D Loss: 1.0911, G Loss: 1.2631\n",
      "Epoch [76/100], Step [500/938], D Loss: 1.3090, G Loss: 1.0470\n",
      "Epoch [76/100], Step [600/938], D Loss: 1.1342, G Loss: 0.8315\n",
      "Epoch [76/100], Step [700/938], D Loss: 1.1059, G Loss: 1.2227\n",
      "Epoch [76/100], Step [800/938], D Loss: 1.1206, G Loss: 1.5858\n",
      "Epoch [76/100], Step [900/938], D Loss: 0.9393, G Loss: 0.9924\n",
      "Epoch [77/100], Step [0/938], D Loss: 1.0307, G Loss: 0.9445\n",
      "Epoch [77/100], Step [100/938], D Loss: 0.9855, G Loss: 1.0781\n",
      "Epoch [77/100], Step [200/938], D Loss: 1.0760, G Loss: 1.4190\n",
      "Epoch [77/100], Step [300/938], D Loss: 1.1189, G Loss: 0.8532\n",
      "Epoch [77/100], Step [400/938], D Loss: 1.0166, G Loss: 1.2195\n",
      "Epoch [77/100], Step [500/938], D Loss: 1.2085, G Loss: 1.3836\n",
      "Epoch [77/100], Step [600/938], D Loss: 1.0538, G Loss: 0.9827\n",
      "Epoch [77/100], Step [700/938], D Loss: 1.1712, G Loss: 2.0082\n",
      "Epoch [77/100], Step [800/938], D Loss: 1.0434, G Loss: 1.2465\n",
      "Epoch [77/100], Step [900/938], D Loss: 1.1839, G Loss: 1.0713\n",
      "Epoch [78/100], Step [0/938], D Loss: 1.0043, G Loss: 1.1025\n",
      "Epoch [78/100], Step [100/938], D Loss: 1.0114, G Loss: 0.8669\n",
      "Epoch [78/100], Step [200/938], D Loss: 1.1145, G Loss: 0.9777\n",
      "Epoch [78/100], Step [300/938], D Loss: 1.2068, G Loss: 1.2601\n",
      "Epoch [78/100], Step [400/938], D Loss: 1.0647, G Loss: 1.2257\n",
      "Epoch [78/100], Step [500/938], D Loss: 1.0556, G Loss: 1.1799\n",
      "Epoch [78/100], Step [600/938], D Loss: 1.0985, G Loss: 0.9287\n",
      "Epoch [78/100], Step [700/938], D Loss: 1.2535, G Loss: 0.8171\n",
      "Epoch [78/100], Step [800/938], D Loss: 1.1625, G Loss: 1.1674\n",
      "Epoch [78/100], Step [900/938], D Loss: 1.1916, G Loss: 1.9818\n",
      "Epoch [79/100], Step [0/938], D Loss: 1.1606, G Loss: 1.4739\n",
      "Epoch [79/100], Step [100/938], D Loss: 1.1741, G Loss: 1.1927\n",
      "Epoch [79/100], Step [200/938], D Loss: 1.0241, G Loss: 1.3443\n",
      "Epoch [79/100], Step [300/938], D Loss: 1.0795, G Loss: 0.9119\n",
      "Epoch [79/100], Step [400/938], D Loss: 0.9478, G Loss: 0.8843\n",
      "Epoch [79/100], Step [500/938], D Loss: 1.0052, G Loss: 1.0127\n",
      "Epoch [79/100], Step [600/938], D Loss: 1.2053, G Loss: 0.9039\n",
      "Epoch [79/100], Step [700/938], D Loss: 1.1007, G Loss: 0.9917\n",
      "Epoch [79/100], Step [800/938], D Loss: 1.1124, G Loss: 1.2334\n",
      "Epoch [79/100], Step [900/938], D Loss: 1.2207, G Loss: 0.9595\n",
      "Epoch [80/100], Step [0/938], D Loss: 1.2212, G Loss: 1.6083\n",
      "Epoch [80/100], Step [100/938], D Loss: 1.1524, G Loss: 1.1985\n",
      "Epoch [80/100], Step [200/938], D Loss: 1.1737, G Loss: 1.6777\n",
      "Epoch [80/100], Step [300/938], D Loss: 1.0310, G Loss: 1.2938\n",
      "Epoch [80/100], Step [400/938], D Loss: 1.1522, G Loss: 0.7914\n",
      "Epoch [80/100], Step [500/938], D Loss: 1.1790, G Loss: 1.4817\n",
      "Epoch [80/100], Step [600/938], D Loss: 1.0774, G Loss: 1.3597\n",
      "Epoch [80/100], Step [700/938], D Loss: 1.0380, G Loss: 1.4728\n",
      "Epoch [80/100], Step [800/938], D Loss: 1.1235, G Loss: 0.8676\n",
      "Epoch [80/100], Step [900/938], D Loss: 0.9804, G Loss: 1.1356\n",
      "Epoch [81/100], Step [0/938], D Loss: 1.0908, G Loss: 1.0267\n",
      "Epoch [81/100], Step [100/938], D Loss: 1.0256, G Loss: 1.1592\n",
      "Epoch [81/100], Step [200/938], D Loss: 1.2068, G Loss: 1.7627\n",
      "Epoch [81/100], Step [300/938], D Loss: 1.0430, G Loss: 1.0849\n",
      "Epoch [81/100], Step [400/938], D Loss: 0.9725, G Loss: 1.2026\n",
      "Epoch [81/100], Step [500/938], D Loss: 1.0657, G Loss: 1.0202\n",
      "Epoch [81/100], Step [600/938], D Loss: 1.1430, G Loss: 1.1496\n",
      "Epoch [81/100], Step [700/938], D Loss: 1.0467, G Loss: 1.0618\n",
      "Epoch [81/100], Step [800/938], D Loss: 1.1945, G Loss: 1.9139\n",
      "Epoch [81/100], Step [900/938], D Loss: 1.1274, G Loss: 1.2729\n",
      "Epoch [82/100], Step [0/938], D Loss: 1.1818, G Loss: 1.2665\n",
      "Epoch [82/100], Step [100/938], D Loss: 1.0148, G Loss: 0.9800\n",
      "Epoch [82/100], Step [200/938], D Loss: 1.1711, G Loss: 1.0992\n",
      "Epoch [82/100], Step [300/938], D Loss: 1.1489, G Loss: 0.9348\n",
      "Epoch [82/100], Step [400/938], D Loss: 1.0227, G Loss: 1.0190\n",
      "Epoch [82/100], Step [500/938], D Loss: 1.0387, G Loss: 1.0543\n",
      "Epoch [82/100], Step [600/938], D Loss: 1.1029, G Loss: 0.7421\n",
      "Epoch [82/100], Step [700/938], D Loss: 1.3040, G Loss: 1.1023\n",
      "Epoch [82/100], Step [800/938], D Loss: 1.1415, G Loss: 1.0294\n",
      "Epoch [82/100], Step [900/938], D Loss: 1.2142, G Loss: 0.9717\n",
      "Epoch [83/100], Step [0/938], D Loss: 0.9988, G Loss: 1.6224\n",
      "Epoch [83/100], Step [100/938], D Loss: 0.9979, G Loss: 0.8051\n",
      "Epoch [83/100], Step [200/938], D Loss: 1.0588, G Loss: 1.1096\n",
      "Epoch [83/100], Step [300/938], D Loss: 0.9166, G Loss: 1.5606\n",
      "Epoch [83/100], Step [400/938], D Loss: 0.9596, G Loss: 1.2746\n",
      "Epoch [83/100], Step [500/938], D Loss: 1.1502, G Loss: 0.4518\n",
      "Epoch [83/100], Step [600/938], D Loss: 1.0478, G Loss: 1.4759\n",
      "Epoch [83/100], Step [700/938], D Loss: 1.0624, G Loss: 1.2288\n",
      "Epoch [83/100], Step [800/938], D Loss: 1.0868, G Loss: 1.2058\n",
      "Epoch [83/100], Step [900/938], D Loss: 1.0405, G Loss: 1.4571\n",
      "Epoch [84/100], Step [0/938], D Loss: 1.2448, G Loss: 1.6842\n",
      "Epoch [84/100], Step [100/938], D Loss: 1.0293, G Loss: 0.9278\n",
      "Epoch [84/100], Step [200/938], D Loss: 1.0159, G Loss: 1.1489\n",
      "Epoch [84/100], Step [300/938], D Loss: 1.1449, G Loss: 0.7449\n",
      "Epoch [84/100], Step [400/938], D Loss: 1.1818, G Loss: 1.9110\n",
      "Epoch [84/100], Step [500/938], D Loss: 1.0687, G Loss: 1.2393\n",
      "Epoch [84/100], Step [600/938], D Loss: 1.0905, G Loss: 1.2041\n",
      "Epoch [84/100], Step [700/938], D Loss: 0.9467, G Loss: 1.0843\n",
      "Epoch [84/100], Step [800/938], D Loss: 1.0999, G Loss: 1.0138\n",
      "Epoch [84/100], Step [900/938], D Loss: 1.1798, G Loss: 1.0604\n",
      "Epoch [85/100], Step [0/938], D Loss: 1.0984, G Loss: 0.9713\n",
      "Epoch [85/100], Step [100/938], D Loss: 1.0283, G Loss: 1.2686\n",
      "Epoch [85/100], Step [200/938], D Loss: 1.1072, G Loss: 1.1851\n",
      "Epoch [85/100], Step [300/938], D Loss: 1.3013, G Loss: 0.9373\n",
      "Epoch [85/100], Step [400/938], D Loss: 1.0909, G Loss: 1.2321\n",
      "Epoch [85/100], Step [500/938], D Loss: 1.1361, G Loss: 0.9948\n",
      "Epoch [85/100], Step [600/938], D Loss: 1.0820, G Loss: 1.1007\n",
      "Epoch [85/100], Step [700/938], D Loss: 1.1594, G Loss: 1.6406\n",
      "Epoch [85/100], Step [800/938], D Loss: 1.0902, G Loss: 0.9491\n",
      "Epoch [85/100], Step [900/938], D Loss: 1.1240, G Loss: 0.9750\n",
      "Epoch [86/100], Step [0/938], D Loss: 1.0989, G Loss: 1.1371\n",
      "Epoch [86/100], Step [100/938], D Loss: 1.1108, G Loss: 1.1686\n",
      "Epoch [86/100], Step [200/938], D Loss: 0.9839, G Loss: 1.1900\n",
      "Epoch [86/100], Step [300/938], D Loss: 1.1129, G Loss: 1.1115\n",
      "Epoch [86/100], Step [400/938], D Loss: 1.0759, G Loss: 1.1137\n",
      "Epoch [86/100], Step [500/938], D Loss: 1.2084, G Loss: 0.8684\n",
      "Epoch [86/100], Step [600/938], D Loss: 1.2396, G Loss: 0.8154\n",
      "Epoch [86/100], Step [700/938], D Loss: 1.1638, G Loss: 0.9163\n",
      "Epoch [86/100], Step [800/938], D Loss: 1.1063, G Loss: 1.0173\n",
      "Epoch [86/100], Step [900/938], D Loss: 1.1344, G Loss: 0.8951\n",
      "Epoch [87/100], Step [0/938], D Loss: 1.2035, G Loss: 1.6456\n",
      "Epoch [87/100], Step [100/938], D Loss: 1.2496, G Loss: 1.6466\n",
      "Epoch [87/100], Step [200/938], D Loss: 0.9270, G Loss: 0.9870\n",
      "Epoch [87/100], Step [300/938], D Loss: 1.0378, G Loss: 1.3835\n",
      "Epoch [87/100], Step [400/938], D Loss: 1.1065, G Loss: 1.2846\n",
      "Epoch [87/100], Step [500/938], D Loss: 1.0405, G Loss: 1.1417\n",
      "Epoch [87/100], Step [600/938], D Loss: 1.0936, G Loss: 1.3733\n",
      "Epoch [87/100], Step [700/938], D Loss: 1.0070, G Loss: 1.0214\n",
      "Epoch [87/100], Step [800/938], D Loss: 1.0700, G Loss: 0.9896\n",
      "Epoch [87/100], Step [900/938], D Loss: 1.1992, G Loss: 1.3997\n",
      "Epoch [88/100], Step [0/938], D Loss: 1.0704, G Loss: 1.4914\n",
      "Epoch [88/100], Step [100/938], D Loss: 1.1047, G Loss: 0.8688\n",
      "Epoch [88/100], Step [200/938], D Loss: 1.0604, G Loss: 1.2756\n",
      "Epoch [88/100], Step [300/938], D Loss: 1.1367, G Loss: 0.9964\n",
      "Epoch [88/100], Step [400/938], D Loss: 1.0019, G Loss: 1.2751\n",
      "Epoch [88/100], Step [500/938], D Loss: 1.0238, G Loss: 1.4918\n",
      "Epoch [88/100], Step [600/938], D Loss: 1.1351, G Loss: 1.1540\n",
      "Epoch [88/100], Step [700/938], D Loss: 1.1591, G Loss: 1.5903\n",
      "Epoch [88/100], Step [800/938], D Loss: 1.3340, G Loss: 0.7452\n",
      "Epoch [88/100], Step [900/938], D Loss: 1.0771, G Loss: 1.5051\n",
      "Epoch [89/100], Step [0/938], D Loss: 1.2118, G Loss: 0.9003\n",
      "Epoch [89/100], Step [100/938], D Loss: 1.0367, G Loss: 1.0611\n",
      "Epoch [89/100], Step [200/938], D Loss: 1.0050, G Loss: 1.1319\n",
      "Epoch [89/100], Step [300/938], D Loss: 1.0348, G Loss: 1.0210\n",
      "Epoch [89/100], Step [400/938], D Loss: 1.0800, G Loss: 1.0725\n",
      "Epoch [89/100], Step [500/938], D Loss: 1.1675, G Loss: 1.0894\n",
      "Epoch [89/100], Step [600/938], D Loss: 1.1296, G Loss: 1.0691\n",
      "Epoch [89/100], Step [700/938], D Loss: 1.2946, G Loss: 1.1769\n",
      "Epoch [89/100], Step [800/938], D Loss: 1.1002, G Loss: 1.3879\n",
      "Epoch [89/100], Step [900/938], D Loss: 1.1805, G Loss: 1.2457\n",
      "Epoch [90/100], Step [0/938], D Loss: 1.1070, G Loss: 0.6921\n",
      "Epoch [90/100], Step [100/938], D Loss: 1.0548, G Loss: 1.3150\n",
      "Epoch [90/100], Step [200/938], D Loss: 1.0629, G Loss: 1.4897\n",
      "Epoch [90/100], Step [300/938], D Loss: 1.0111, G Loss: 1.3256\n",
      "Epoch [90/100], Step [400/938], D Loss: 1.2397, G Loss: 1.2093\n",
      "Epoch [90/100], Step [500/938], D Loss: 1.1787, G Loss: 1.4952\n",
      "Epoch [90/100], Step [600/938], D Loss: 1.1362, G Loss: 0.9847\n",
      "Epoch [90/100], Step [700/938], D Loss: 1.1026, G Loss: 1.1209\n",
      "Epoch [90/100], Step [800/938], D Loss: 0.9844, G Loss: 1.2437\n",
      "Epoch [90/100], Step [900/938], D Loss: 1.1690, G Loss: 0.9098\n",
      "Epoch [91/100], Step [0/938], D Loss: 1.1700, G Loss: 1.3855\n",
      "Epoch [91/100], Step [100/938], D Loss: 1.0988, G Loss: 0.8531\n",
      "Epoch [91/100], Step [200/938], D Loss: 1.1799, G Loss: 0.9938\n",
      "Epoch [91/100], Step [300/938], D Loss: 1.1052, G Loss: 1.5495\n",
      "Epoch [91/100], Step [400/938], D Loss: 1.0943, G Loss: 1.3757\n",
      "Epoch [91/100], Step [500/938], D Loss: 1.1438, G Loss: 0.7128\n",
      "Epoch [91/100], Step [600/938], D Loss: 1.0361, G Loss: 1.3473\n",
      "Epoch [91/100], Step [700/938], D Loss: 1.2185, G Loss: 0.8403\n",
      "Epoch [91/100], Step [800/938], D Loss: 1.2000, G Loss: 0.8113\n",
      "Epoch [91/100], Step [900/938], D Loss: 1.1089, G Loss: 0.9623\n",
      "Epoch [92/100], Step [0/938], D Loss: 1.1399, G Loss: 1.6166\n",
      "Epoch [92/100], Step [100/938], D Loss: 1.0802, G Loss: 1.2683\n",
      "Epoch [92/100], Step [200/938], D Loss: 1.0332, G Loss: 1.0159\n",
      "Epoch [92/100], Step [300/938], D Loss: 1.0438, G Loss: 1.1076\n",
      "Epoch [92/100], Step [400/938], D Loss: 0.9485, G Loss: 1.0461\n",
      "Epoch [92/100], Step [500/938], D Loss: 0.9876, G Loss: 1.1580\n",
      "Epoch [92/100], Step [600/938], D Loss: 1.1187, G Loss: 1.6802\n",
      "Epoch [92/100], Step [700/938], D Loss: 1.0219, G Loss: 1.2339\n",
      "Epoch [92/100], Step [800/938], D Loss: 1.1550, G Loss: 1.7200\n",
      "Epoch [92/100], Step [900/938], D Loss: 1.1507, G Loss: 0.8208\n",
      "Epoch [93/100], Step [0/938], D Loss: 1.2667, G Loss: 0.9600\n",
      "Epoch [93/100], Step [100/938], D Loss: 1.0852, G Loss: 1.0186\n",
      "Epoch [93/100], Step [200/938], D Loss: 1.1615, G Loss: 1.3373\n",
      "Epoch [93/100], Step [300/938], D Loss: 1.1195, G Loss: 1.0163\n",
      "Epoch [93/100], Step [400/938], D Loss: 1.0914, G Loss: 1.1066\n",
      "Epoch [93/100], Step [500/938], D Loss: 1.0568, G Loss: 1.1405\n",
      "Epoch [93/100], Step [600/938], D Loss: 1.0610, G Loss: 1.2943\n",
      "Epoch [93/100], Step [700/938], D Loss: 1.0958, G Loss: 0.9475\n",
      "Epoch [93/100], Step [800/938], D Loss: 1.1033, G Loss: 1.1085\n",
      "Epoch [93/100], Step [900/938], D Loss: 1.0952, G Loss: 0.8229\n",
      "Epoch [94/100], Step [0/938], D Loss: 1.1211, G Loss: 1.4818\n",
      "Epoch [94/100], Step [100/938], D Loss: 1.1605, G Loss: 2.0462\n",
      "Epoch [94/100], Step [200/938], D Loss: 1.0837, G Loss: 1.5276\n",
      "Epoch [94/100], Step [300/938], D Loss: 1.0555, G Loss: 0.9663\n",
      "Epoch [94/100], Step [400/938], D Loss: 1.0813, G Loss: 1.4437\n",
      "Epoch [94/100], Step [500/938], D Loss: 0.9766, G Loss: 1.1632\n",
      "Epoch [94/100], Step [600/938], D Loss: 1.1119, G Loss: 0.9458\n",
      "Epoch [94/100], Step [700/938], D Loss: 0.9468, G Loss: 0.9916\n",
      "Epoch [94/100], Step [800/938], D Loss: 1.1231, G Loss: 0.7286\n",
      "Epoch [94/100], Step [900/938], D Loss: 1.0984, G Loss: 0.8016\n",
      "Epoch [95/100], Step [0/938], D Loss: 1.0424, G Loss: 1.0866\n",
      "Epoch [95/100], Step [100/938], D Loss: 1.1808, G Loss: 1.0380\n",
      "Epoch [95/100], Step [200/938], D Loss: 1.0267, G Loss: 1.2839\n",
      "Epoch [95/100], Step [300/938], D Loss: 1.1038, G Loss: 1.7525\n",
      "Epoch [95/100], Step [400/938], D Loss: 1.0989, G Loss: 1.1658\n",
      "Epoch [95/100], Step [500/938], D Loss: 1.1650, G Loss: 1.0063\n",
      "Epoch [95/100], Step [600/938], D Loss: 1.0727, G Loss: 0.9890\n",
      "Epoch [95/100], Step [700/938], D Loss: 1.0967, G Loss: 1.3632\n",
      "Epoch [95/100], Step [800/938], D Loss: 1.0414, G Loss: 1.4883\n",
      "Epoch [95/100], Step [900/938], D Loss: 1.1136, G Loss: 0.9911\n",
      "Epoch [96/100], Step [0/938], D Loss: 0.9701, G Loss: 0.7829\n",
      "Epoch [96/100], Step [100/938], D Loss: 0.8968, G Loss: 1.3538\n",
      "Epoch [96/100], Step [200/938], D Loss: 1.1404, G Loss: 1.3318\n",
      "Epoch [96/100], Step [300/938], D Loss: 1.1151, G Loss: 0.7469\n",
      "Epoch [96/100], Step [400/938], D Loss: 1.0098, G Loss: 1.3802\n",
      "Epoch [96/100], Step [500/938], D Loss: 1.1029, G Loss: 1.1047\n",
      "Epoch [96/100], Step [600/938], D Loss: 1.0437, G Loss: 1.1021\n",
      "Epoch [96/100], Step [700/938], D Loss: 1.2318, G Loss: 0.8807\n",
      "Epoch [96/100], Step [800/938], D Loss: 1.0887, G Loss: 1.1735\n",
      "Epoch [96/100], Step [900/938], D Loss: 1.0940, G Loss: 0.7072\n",
      "Epoch [97/100], Step [0/938], D Loss: 1.0554, G Loss: 1.2231\n",
      "Epoch [97/100], Step [100/938], D Loss: 1.1064, G Loss: 1.0100\n",
      "Epoch [97/100], Step [200/938], D Loss: 1.0585, G Loss: 1.0829\n",
      "Epoch [97/100], Step [300/938], D Loss: 1.0950, G Loss: 1.2537\n",
      "Epoch [97/100], Step [400/938], D Loss: 1.0238, G Loss: 1.1401\n",
      "Epoch [97/100], Step [500/938], D Loss: 1.0193, G Loss: 1.1410\n",
      "Epoch [97/100], Step [600/938], D Loss: 1.0753, G Loss: 0.9497\n",
      "Epoch [97/100], Step [700/938], D Loss: 0.9873, G Loss: 0.9662\n",
      "Epoch [97/100], Step [800/938], D Loss: 1.0132, G Loss: 1.1098\n",
      "Epoch [97/100], Step [900/938], D Loss: 1.0660, G Loss: 1.4781\n",
      "Epoch [98/100], Step [0/938], D Loss: 1.0680, G Loss: 1.2297\n",
      "Epoch [98/100], Step [100/938], D Loss: 1.2205, G Loss: 0.7057\n",
      "Epoch [98/100], Step [200/938], D Loss: 1.4254, G Loss: 1.7623\n",
      "Epoch [98/100], Step [300/938], D Loss: 1.1917, G Loss: 1.2381\n",
      "Epoch [98/100], Step [400/938], D Loss: 1.0769, G Loss: 0.9122\n",
      "Epoch [98/100], Step [500/938], D Loss: 1.0675, G Loss: 1.2491\n",
      "Epoch [98/100], Step [600/938], D Loss: 1.1241, G Loss: 0.7382\n",
      "Epoch [98/100], Step [700/938], D Loss: 1.2288, G Loss: 0.7767\n",
      "Epoch [98/100], Step [800/938], D Loss: 1.2612, G Loss: 1.6649\n",
      "Epoch [98/100], Step [900/938], D Loss: 1.2561, G Loss: 1.3402\n",
      "Epoch [99/100], Step [0/938], D Loss: 1.0575, G Loss: 1.5422\n",
      "Epoch [99/100], Step [100/938], D Loss: 1.0763, G Loss: 1.2059\n",
      "Epoch [99/100], Step [200/938], D Loss: 1.0165, G Loss: 1.4618\n",
      "Epoch [99/100], Step [300/938], D Loss: 1.0056, G Loss: 1.3043\n",
      "Epoch [99/100], Step [400/938], D Loss: 1.2428, G Loss: 0.5470\n",
      "Epoch [99/100], Step [500/938], D Loss: 1.1307, G Loss: 1.2275\n",
      "Epoch [99/100], Step [600/938], D Loss: 1.1318, G Loss: 1.2336\n",
      "Epoch [99/100], Step [700/938], D Loss: 1.0045, G Loss: 0.8743\n",
      "Epoch [99/100], Step [800/938], D Loss: 1.1462, G Loss: 1.3819\n",
      "Epoch [99/100], Step [900/938], D Loss: 1.2300, G Loss: 0.7258\n",
      "Epoch [100/100], Step [0/938], D Loss: 1.0714, G Loss: 0.8644\n",
      "Epoch [100/100], Step [100/938], D Loss: 1.1942, G Loss: 1.0047\n",
      "Epoch [100/100], Step [200/938], D Loss: 1.0845, G Loss: 1.8217\n",
      "Epoch [100/100], Step [300/938], D Loss: 1.2384, G Loss: 1.1589\n",
      "Epoch [100/100], Step [400/938], D Loss: 1.0569, G Loss: 1.2013\n",
      "Epoch [100/100], Step [500/938], D Loss: 1.1314, G Loss: 1.4422\n",
      "Epoch [100/100], Step [600/938], D Loss: 1.0475, G Loss: 1.0779\n",
      "Epoch [100/100], Step [700/938], D Loss: 1.1604, G Loss: 0.9767\n",
      "Epoch [100/100], Step [800/938], D Loss: 0.9809, G Loss: 1.0271\n",
      "Epoch [100/100], Step [900/938], D Loss: 1.2456, G Loss: 1.3971\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Labels\n",
    "        real_labels = torch.ones(batch_size, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size, 1, device=device)\n",
    "        \n",
    "        ### Train Discriminator ##\n",
    "        discriminator_optimizer.zero_grad()\n",
    "        outputs = discriminator(real_images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        ### Train Generator ##\n",
    "        generator_optimizer.zero_grad()\n",
    "        noise = torch.randn(batch_size, noise_dim, device=device)\n",
    "        fake_images = generator(noise)\n",
    "        outputs = discriminator(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)  # Fool the discriminator!\n",
    "        g_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb5fa4-1113-470e-b2f1-6abf09745c41",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f305a2d-9d9b-4547-bd6b-c3fbd024d15f",
   "metadata": {},
   "source": [
    "## 8. Generate and Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24ff874d-ce63-4ad4-a73c-f66233fbf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "noise = torch.randn(16, noise_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e76ae11e-c03a-4739-8e43-a8b5aef3f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images = generator(noise)\n",
    "fake_images = fake_images.view(-1, 1, 28, 28).cpu().detach()\n",
    "fake_images = (fake_images + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af679501-3e46-4ccb-b292-c2ee7903655b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "for i in range(16):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.imshow(fake_images[i][0], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5710d-cd58-4e7c-b8b5-5c2ac3ff33c7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef8b52-fbb8-40af-a8bb-2c6e5148cccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
